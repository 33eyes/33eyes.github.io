<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Arina Igumenshcheva</title><link href="http://33eyes.github.io/" rel="alternate"></link><link href="http://33eyes.github.io/feeds/all.atom.xml" rel="self"></link><id>http://33eyes.github.io/</id><updated>2018-10-20T12:19:00-04:00</updated><entry><title>GraphConnect NYC 2018</title><link href="http://33eyes.github.io/2018-10-20-GraphConnect-NYC-2018.html" rel="alternate"></link><published>2018-10-20T12:19:00-04:00</published><updated>2018-10-20T12:19:00-04:00</updated><author><name>Arina Igumenshcheva</name></author><id>tag:33eyes.github.io,2018-10-20:/2018-10-20-GraphConnect-NYC-2018.html</id><summary type="html">&lt;p&gt;About a month ago, I've attended the &lt;a href="https://graphconnect.com/"&gt;GraphConnect conference&lt;/a&gt; in NYC.
I had a great time at the conference, and thought I'd share my experience, along with a brief intro to graph concepts and technologies.&lt;/p&gt;
&lt;h2&gt;About the conference&lt;/h2&gt;
&lt;h5&gt;&lt;strong&gt;Day 1: Sessions&lt;/strong&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=dW6JsFccdkM"&gt;Keynote speeches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Talks about applications of graph databases and …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;About a month ago, I've attended the &lt;a href="https://graphconnect.com/"&gt;GraphConnect conference&lt;/a&gt; in NYC.
I had a great time at the conference, and thought I'd share my experience, along with a brief intro to graph concepts and technologies.&lt;/p&gt;
&lt;h2&gt;About the conference&lt;/h2&gt;
&lt;h5&gt;&lt;strong&gt;Day 1: Sessions&lt;/strong&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=dW6JsFccdkM"&gt;Keynote speeches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Talks about applications of graph databases and graph analytics across a
  wide gamut of industries, including:&lt;ul&gt;
&lt;li&gt;Fraud detection&lt;/li&gt;
&lt;li&gt;Healthcare IT&lt;/li&gt;
&lt;li&gt;Medical and scientific research&lt;/li&gt;
&lt;li&gt;Web development&lt;/li&gt;
&lt;li&gt;AI and ML&lt;/li&gt;
&lt;li&gt;GDPR compliance&lt;/li&gt;
&lt;li&gt;Master data management&lt;/li&gt;
&lt;li&gt;Identity and access management&lt;/li&gt;
&lt;li&gt;HR&lt;/li&gt;
&lt;li&gt;Finance&lt;/li&gt;
&lt;li&gt;And anything that relies on recommender engines&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Neo4j-led sessions covering product features, use cases and new releases&lt;ul&gt;
&lt;li&gt;2018 update highlights:&lt;ul&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/bloom/"&gt;Neo4j Bloom&lt;/a&gt;: a visual graph exploration application&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/neo4j/whats-new-in-neo4j-spatial-features-586d69cda8d0"&gt;Spatial features in Neo4j&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.adamcowley.co.uk/neo4j/temporal-native-dates/"&gt;Temporal date types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/"&gt;Graph algorithms library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/blog/apoc-release-for-neo4j-3-4-with-graph-grouping/"&gt;An updated APOC library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=NMDdVH9hHYo"&gt;Neo4j Morpheus&lt;/a&gt;: combines graph and relational table functionalities into one application&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=OlTtBG6mQbc"&gt;2018 Graphie Awards&lt;/a&gt;: a recognition of companies and graph community members for their contributions to the graph tech community  &lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;strong&gt;Day 2: Training&lt;/strong&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Code-along training workshops in Neo4j  &lt;/li&gt;
&lt;li&gt;Included workshops on:  &lt;ul&gt;
&lt;li&gt;Natural language understanding  &lt;/li&gt;
&lt;li&gt;Graph modeling  &lt;/li&gt;
&lt;li&gt;Data science applications  &lt;/li&gt;
&lt;li&gt;Graph algorithms  &lt;/li&gt;
&lt;li&gt;Full stack development with &lt;a href="https://grandstack.io/"&gt;GRANDstack&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;strong&gt;Day 3: GraphHack!&lt;/strong&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;A graph tech hackathon&lt;/li&gt;
&lt;li&gt;It was really fun, and great opportunity to try out awesome graph tech!&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;strong&gt;Throughout the conference&lt;/strong&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1-1 graph clinics&lt;/strong&gt;: you could schedule a 15 min meeting with a Neo4j expert to get help with Neo4j or graph tech in general  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expo&lt;/strong&gt;: a showcase of businesses that either leverage or support Neo4j solutions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;About Neo4j&lt;/h2&gt;
&lt;p&gt;GraphConnect was organized by &lt;a href="https://neo4j.com/"&gt;Neo4j&lt;/a&gt;, the company that makes the Neo4j graph database platform. Neo4j was formed way back when it was cool to name things after 'The Matrix' movies, and it has been leading the way in graph tech ever since.  &lt;/p&gt;
&lt;p&gt;Originally, Neo4j's founders were trying to solve a specific data problem, caused by inefficient merges of relational database tables. Ironically, the relational database structure was preventing them from efficiently accessing and operating on the relationships in the data. When they realized that, they set out to build a database structure where data relationships are a first-class entity.&lt;br&gt;
And build it they did. The Neo4j graph database they've built was a fast back-end for applications that derive value from highly connected data, like product recommendations on online retailer websites, flight fares and hotel bookings, and so on.  &lt;/p&gt;
&lt;p&gt;But that didn't end there. As people started to experiment with graph databases more and more, some started using Neo4j for data analytics, taxonomy explorations, feature engineering, and lots of other applications. Through engaging with this graphs community, Neo4j has gone on to create new graph analytics solutions, ranging from analytics-friendly data visualization to complex graph algorithms made easy and fast.  &lt;/p&gt;
&lt;p&gt;So now, the Neo4j graph database is really a two-pronged product. On the one hand, it offers developers a fast database back-end for highly connected data. And on the other hand, it is a powerful tool for graph analytics and connected data exploration, which greatly benefits researchers, data scientists, business analysts, and anyone else who is trying to derive insights from highly connected data.  &lt;/p&gt;
&lt;h2&gt;A very brief intro to graphs&lt;/h2&gt;
&lt;h5&gt;&lt;strong&gt;What is a graph?&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;It's just a way to think of and represent interconnected things. A graph can also be called a network, the two terms are equivalent.&lt;br&gt;
Graphs are made of &lt;strong&gt;nodes&lt;/strong&gt; (the interconnected things themselves) and the &lt;strong&gt;relationships&lt;/strong&gt; between those things. Relationships are usually represented by lines/arrows.&lt;br&gt;
A very basic graph can be just two nodes connected by a relationship. If we have a relationship where the direction of it matters, then we have a &lt;strong&gt;directional graph&lt;/strong&gt;. We can also attach various properties to both nodes and relationships. Such graphs are called &lt;strong&gt;property graphs&lt;/strong&gt;. &lt;br&gt;
&lt;div class="row"&gt;
  &lt;div class="col" style="text-align: center;"&gt;
    &lt;img alt="basic graph types" style="width:350px;" src="/images/basic_graph_types_sm2.png"&gt;
  &lt;/div&gt;
  &lt;div class="col" style="text-align: center;"&gt;
  &lt;img alt="graph with many node and relationship types" style="width:350px;" src="/images/graph_with_many_types_sm.png"&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Many different kinds of nodes and relationships are possible in a graph. The exact nature of nodes and how they are connected determines what calculations can be done on the graph.  &lt;/p&gt;
&lt;p&gt;Neo4j's graph database allows both directed relationships and properties on a graph,
so it can be used to model data as a directed property graph.&lt;/p&gt;
&lt;p&gt;When our connected data is modeled as a graph, we can leverage graph structure properties to either
optimize connected data processing in real-time solutions (a use case for devs), or derive new insights (a use case for researchers, analysts and data scientists). Or, we might just visualize the data graph as is, and visually investigate the data. The graph data representation is intuitive enough for anyone to just look at it and gain
some understanding of the data.&lt;/p&gt;
&lt;h2&gt;&lt;code&gt;(graphs)-[:ARE]-&amp;gt;(everywhere)&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;According to the CEO and co-founder of Neo4j Emil Eifrem, if you've booked a
hotel room, browsed airfare online, or even used an ATM recently, then you've
most likely used Neo4j without even realizing it.
Insurance companies use graph databases to make the most of their connected data,
and many of them choose Neo4j to do that.
Fraud detection is at this point a classic problem to tackle with graph tech, and lots of
large financial institutions use graph tech to make financial data connections
transparent enough so that fraudulent behavior can't hide within big data.&lt;br&gt;
Those are just the larger graph-tech-using businesses that people are likely to run into on a daily basis, but there are many other industries and businesses that are using graph technologies as well, ranging from retail to healthcare to scientific research.  &lt;/p&gt;
&lt;h2&gt;Graphs + data science = new insights into connected data&lt;/h2&gt;
&lt;p&gt;Graphs are an incredibly powerful data tool that spans many industries and techniques.
I think that graphs are going to become an essential tool for data scientists in the near future. While a graph approach is not needed in every data-driven research or statistical analysis project, more and more of the big data sources and challenges involve connected data. From wearable fitness trackers to online consumer behavior, to drug interactions, to movie recommendations, to scientific research databases, people are constantly creating vast amounts of interconnected data. As data scientists, we are tasked with extracting insights from that data, and we can gain novel insights from the relationship structures within the interconnected data by adding graph techniques to our data science toolkits.  &lt;/p&gt;
&lt;p&gt;Right now, there are 3 main ways that any data scientist working with highly interconnected data can immediately gain value from using graph tech: visual exploration of connections within the data, graph algorithms, and feature extraction.  &lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Visual exploration of connected data&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Visual data exploration is a simple yet powerful tool in data science.
It can help data scientists better understand their data and make informed decisions when planning their approach to analyzing the data. Visualization also helps data scientists communicate insights about the data to less technical stakeholders.  &lt;/p&gt;
&lt;p&gt;When it comes to highly interconnected data, graph-based data visualization takes on an interesting role, because in some data-driven industries a custom visualization of data interconnections is the final data analytics product. For example, in fraud detection the visualization of connections between financial transactions, businesses and individuals is used by fraud investigators to visually inspect the data an find suspicious behavior. Another example is social network analytics, where an analyst might visually inspect the impact of a social media influencer on a marketing campaign.&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Graph algorithms&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Graph algorithms can be used to quantify graph structures within the connected data. Currently there are 3 main kinds of graph algorithms: centrality, community detection, and pathfinding.&lt;/p&gt;
&lt;h6&gt;&lt;strong&gt;Centrality algorithms&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;Centrality algorithms measure the relationship-based importance of individual nodes within a network. They assign a computed metric to each node, ranking its importance in the graph.  &lt;/p&gt;
&lt;h6&gt;Typical questions these algorithms can help answer:&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Who are the key influencers in your connected data?  &lt;/li&gt;
&lt;li&gt;Who has the most connections?  &lt;/li&gt;
&lt;li&gt;Who is directly or indirectly connected the largest proportion of the network?&lt;/li&gt;
&lt;li&gt;Who connects distinct groups of nodes that are otherwise not connected to each other?&lt;/li&gt;
&lt;li&gt;If a specific node is to be changed or removed, how important is this to the network? How many of the other nodes will be affected, and how far can these effects propagate through the graph?&lt;/li&gt;
&lt;/ul&gt;
&lt;h6&gt;Algorithm examples:&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/algorithms/page-rank/"&gt;PageRank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/algorithms/betweenness-centrality/"&gt;Betweenness centrality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/algorithms/closeness-centrality/"&gt;Closeness centrality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6&gt;&lt;strong&gt;Community detection&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;Community detection algorithms are used to cluster data based on the distribution of relationships in a graph. The resulting clusters of nodes are the sub-communities within the whole network, where the nodes within the communities have stronger relationship ties to each other than to the nodes in other communities.&lt;/p&gt;
&lt;h6&gt;Typical questions these algorithms can help answer:&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Are there communities in your connected data? What are they?&lt;/li&gt;
&lt;li&gt;Are there hierarchical communities?&lt;/li&gt;
&lt;li&gt;Are all of the nodes in the network connected?&lt;/li&gt;
&lt;/ul&gt;
&lt;h6&gt;Algorithm examples:&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/algorithms/label-propagation/"&gt;Label propagation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/algorithms/louvain/"&gt;Louvain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/algorithms/triangle-counting-clustering-coefficient/"&gt;Triangle counting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/algorithms/connected-components/"&gt;Connected components&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6&gt;&lt;strong&gt;Pathfinding&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;These algorithms find and evaluate paths in a network. They can help answer a wide range of questions related to traversing a path in a graph along nodes and relationships. This set of algorithms includes a random walk long the graph.&lt;/p&gt;
&lt;h6&gt;Typical questions these algorithms can help answer:&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;What's the shortest path from node A to node B?&lt;/li&gt;
&lt;li&gt;If we start at node A, where can we go in the graph? What are the possible paths?&lt;/li&gt;
&lt;li&gt;What can we learn about our network by going for a random walk on it?&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Six_degrees_of_separation"&gt;6 degrees of separation&lt;/a&gt;-based questions&lt;/li&gt;
&lt;/ul&gt;
&lt;h6&gt;Algorithm examples:&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/algorithms/shortest-path/"&gt;Shortest path&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/algorithms/minimum-weight-spanning-tree/"&gt;Minimum weight spanning tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/algorithms/random-walk/"&gt;Random walk&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;strong&gt;Feature extraction&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Feature extraction in data science can mean two things. There is the more general definition of deriving a set of features from input data, and there is a more specific definition in machine learning as a dimensionality reduction technique. I'm going with the more general definition first here, and then with the machine learning one.&lt;/p&gt;
&lt;h6&gt;&lt;strong&gt;Graph metrics and properties as features&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;Graph algorithms are useful analytic tools on their own, but they can also be used to extract features from the graph that can then be fed into more traditional data science techniques. For a simple example, we can run a community detection algorithm on our connected data, and then extract the found communities as a categorical variable to be used in regression analysis.&lt;br&gt;
Similarly, we can extract any kind of localized graph feature associated with a node and use it in further analyses.  &lt;/p&gt;
&lt;h6&gt;&lt;strong&gt;Graph embeddings&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;We can also use graph embeddings, similar to word embeddings like word2vec, to automatically extract graph-based features to feed into machine learning models. &lt;a href="https://towardsdatascience.com/deepgl-on-neo4j-b27e8c64190f"&gt;Mark Needham has a great article on this&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Neo4j resources for data scientists&lt;/strong&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/graphgists/"&gt;GraphGists&lt;/a&gt;: explore sample graph data projects  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://community.neo4j.com"&gt;Community forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/graph-algorithms/current/introduction/"&gt;Graph algorithms docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/channel/UCvze3hU6OZBkB1vkhH2lH9Q"&gt;Neo4j's youtube channel&lt;/a&gt; has lots of tutorials and conference/webinar recordings  &lt;ul&gt;
&lt;li&gt;For data scientists working with customer journey data, I highly recommend this &lt;a href="https://www.youtube.com/watch?v=Rlh9PkHinAs"&gt;overview of graph-based customer journey analytics&lt;/a&gt;!  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;div class="row" markdown="1"&gt;
&lt;div class="col-sm-10 offset-sm-1"&gt;
&lt;p style="color: #818a91;
    font-style: italic;
    font-size: 1.4em;
    text-align: left;
    font-weight: bold;"&gt;"Graph analysis is possibly the single most effective competitive differentiator for organizations pursuing data-driven operations and decisions."&lt;br&gt;
&lt;/p&gt;
&lt;p style="color: #818a91;
    font-style: italic;
    text-align:right"&gt;
- Gartner Research
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Book recommendations&lt;/h2&gt;
&lt;p&gt;The conference had some interesting book recommendations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/dp/B002OFVO5Y"&gt;"Connected" by James Fowler&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.oreilly.com/library/view/ethics-and-data/9781492043898/"&gt;Oreilly's "Ethics and Data Science" by DJ Patil, Hilary Mason, Mike Loukides&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Science-Design-Manual-Texts-Computer/dp/3319554433"&gt;"The Data Science Design Manual" by Steven Skiena&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Algorithm-Design-Manual-Steven-Skiena-dp-1848000693/dp/1848000693/"&gt;"The Algorithm Design Manual" by Steven Skiena&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Software-Paradox-Rise-Commercial-Market/dp/1491900938"&gt;"The Software Paradox" by Stephen O'Grady&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="graph-databases"></category><category term="neo4j"></category><category term="Cypher"></category><category term="network-analysis"></category></entry><entry><title>Exploring Wikipedia clickstream data, part 2</title><link href="http://33eyes.github.io/2018-4-20-Wikipedia-clickstream-2.html" rel="alternate"></link><published>2018-04-20T10:20:00-04:00</published><updated>2018-04-20T10:20:00-04:00</updated><author><name>Arina Igumenshcheva</name></author><id>tag:33eyes.github.io,2018-04-20:/2018-4-20-Wikipedia-clickstream-2.html</id><summary type="html">&lt;p&gt;In the &lt;a href="http://www.arigu.me/2018-4-16-Wikipedia-clickstream-1.html"&gt;previous blog post&lt;/a&gt;, I've covered what Wikipedia clickstream data is, why we may want to explore it using network analysis, and how to load that data into a neo4j database. Here's a brief recap:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wikipedia clickstream data is data about which Wikipedia articles the users visit, and how …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;In the &lt;a href="http://www.arigu.me/2018-4-16-Wikipedia-clickstream-1.html"&gt;previous blog post&lt;/a&gt;, I've covered what Wikipedia clickstream data is, why we may want to explore it using network analysis, and how to load that data into a neo4j database. Here's a brief recap:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wikipedia clickstream data is data about which Wikipedia articles the users visit, and how they get to Wikipedia. This data is at aggregate level across users per website-to-article hop.&lt;/li&gt;
&lt;li&gt;The data I am using for this project is the March 2018 Wikipedia clickstream data release. This means that my dataset contains data on how the users traversed Wikipedia during March 2018. &lt;/li&gt;
&lt;li&gt;Wikipedia articles have lots of links to other articles, so the Wikipedia clickstream data is highly interconnected. These connections are best modeled and explored as a graph, and for this purpose we've loaded the data into the neo4j graph database.&lt;/li&gt;
&lt;li&gt;The March 2018 Wikipedia clickstream data release provides separate datasets for 11 Wikipedia language domains. These language domains are separate Wikipedia projects, and their data is not interlinked article-to-article in the data release. As a result, we have loaded the data for each language domain as a separate graph.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, I will simply describe and compare the Wikipedia clickstream data graphs across 11 language domains. Further analyses will be continued in the next post.&lt;/p&gt;
&lt;h1&gt;Network properties&lt;/h1&gt;
&lt;p&gt;Network properties are certain graph attributes that can be calculated to get a sense of the structure of the network. These properties can be used to compare networks to each other and to graph models.  &lt;/p&gt;
&lt;h2&gt;Webpage-to-article clickstream graph&lt;/h2&gt;
&lt;p&gt;First, lets take a look at the entire data that we have loaded into separate graphs per language domains. This data includes both Article nodes and ExternalSource nodes. ExternalSource nodes are just categories of external webpages from which users came to Wikipedia articles, and they tend to be connected to lots of articles.  &lt;/p&gt;
&lt;h3&gt;Graph size&lt;/h3&gt;
&lt;p&gt;Graph size is the number of nodes and/or edges in a network. In the webpage-to-article graph, we have two kinds of nodes: Articles and ExternalSources. Each language domain graph has 5 ExternalSource nodes, and we can calculate the counts of Article nodes and all edges in the graph.  &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;language domain&lt;/th&gt;
&lt;th&gt;articles count&lt;/th&gt;
&lt;th&gt;external sources count&lt;/th&gt;
&lt;th&gt;edges count&lt;/th&gt;
&lt;th&gt;references count&lt;/th&gt;
&lt;th&gt;average references per edge&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English (EN)&lt;/td&gt;
&lt;td&gt;4,636,312&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;25,917,378&lt;/td&gt;
&lt;td&gt;7,209,691,324&lt;/td&gt;
&lt;td&gt;278.18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Russian (RU)&lt;/td&gt;
&lt;td&gt;1,310,230&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2,712,051&lt;/td&gt;
&lt;td&gt;704,827,086&lt;/td&gt;
&lt;td&gt;259.89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;German (DE)&lt;/td&gt;
&lt;td&gt;1,307,048&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;5,305,000&lt;/td&gt;
&lt;td&gt;775,318,583&lt;/td&gt;
&lt;td&gt;146.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;French (FR)&lt;/td&gt;
&lt;td&gt;1,018,507&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;3,545,691&lt;/td&gt;
&lt;td&gt;584,620,558&lt;/td&gt;
&lt;td&gt;164.88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Japanese (JA)&lt;/td&gt;
&lt;td&gt;880,747&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2,295,941&lt;/td&gt;
&lt;td&gt;765,429,220&lt;/td&gt;
&lt;td&gt;333.38&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spanish (ES)&lt;/td&gt;
&lt;td&gt;802,360&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;3,409,055&lt;/td&gt;
&lt;td&gt;915,554,698&lt;/td&gt;
&lt;td&gt;268.57&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Italian (IT)&lt;/td&gt;
&lt;td&gt;698,313&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2,919,343&lt;/td&gt;
&lt;td&gt;476,970,761&lt;/td&gt;
&lt;td&gt;163.38&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Portuguese (PT)&lt;/td&gt;
&lt;td&gt;523,616&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1,773,920&lt;/td&gt;
&lt;td&gt;315,514,276&lt;/td&gt;
&lt;td&gt;177.86&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chinese (ZH)&lt;/td&gt;
&lt;td&gt;497,074&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1,238,296&lt;/td&gt;
&lt;td&gt;291,813,076&lt;/td&gt;
&lt;td&gt;235.66&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Polish (PL)&lt;/td&gt;
&lt;td&gt;493,158&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1,556,704&lt;/td&gt;
&lt;td&gt;202,264,461&lt;/td&gt;
&lt;td&gt;129.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Persian (FA)&lt;/td&gt;
&lt;td&gt;170,187&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;375,565&lt;/td&gt;
&lt;td&gt;74,059,745&lt;/td&gt;
&lt;td&gt;197.20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the graph size summary table above, we can see that the English language domain Wikipedia clickstream graph is the largest, with the highest counts of nodes and edges traversed in March 2018. This makes sense, since the English Wikipedia was the first Wikipedia project, and a large proportion of users browse the web using English.  &lt;/p&gt;
&lt;p&gt;Plotting the language domain graph sizes measured by article count, we can see the dramatic difference between the English domain Wikipedia and all other language domains. The English domain Wikipedia users browsed through 4.6 million articles in March 2018. Russian and German domain Wikipedia users have browsed 1.3 million articles each, followed by French, Japanese, Spanish and Italian domain users who have browsed in the 0.7 to 1 million articles range each, followed by Portuguese, Chinese and Polish domain Wikipedia browsing of about half a million articles each. After that, there's another proportionally large drop off in articles browsed in the Persian domain Wikipedia, followed by all other language domain Wikipedias not listed here, since we only have the clickstream data for the largest 11 language domains.  &lt;/p&gt;
&lt;div class="centered"&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream graph sizes by language domain&lt;/strong&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;div class="centered"&gt;
&lt;p&gt;&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_graph_size_barchart.png"&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;If we look at the counts of edges as an additional measure of graph sizes, we see a very similar pattern overall, with the exception of Russian and Japanese language domains, which seem to have relatively low edge counts given their article counts, compared to other language domains. &lt;br&gt;
&lt;div class="centered" markdown="1"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream graph edge counts by language domain&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;/div&gt;
&lt;div class="centered" markdown="1"&gt;
&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_graph_edges_count_barchart.png"&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Edges connect Wikipedia article nodes, and the number of all possible directed edges in a graph is &lt;strong&gt;N(N-1)&lt;/strong&gt;, where &lt;strong&gt;N&lt;/strong&gt; is the number of nodes. The more article nodes we have in a graph, the more edges we can have. But different user clickstream behaviors can result in graph structures with higher or lower actual edge counts relative to the number of all possible edges. So the relative dips in edge counts may suggest structural differences in graphs and underlying user clickstream behaviors.&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Now let's take a look at the distribution of references across the 11 language domains. References here are the individual hops by users from a webpage (an external website or a Wikipedia article) to a Wikipedia article, and their counts are the weights on the edges of our 11 weighted graphs. Again, we'd expect there to be more references in the language domain graphs that have more edges. &lt;/p&gt;
&lt;div class="centered"&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream reference counts by language domain&lt;/strong&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;div class="centered"&gt;
&lt;p&gt;&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_graph_references_count_barchart.png"&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As expected, the references count for English domain Wikipedia is much greater than for the other language domains. But if we look at the next top 5 language domains by reference count, they are all approximately in 600-900 million references range, with Spanish Wikipedia having the 2nd highest references count of 916 million. This is surprisingly uniform considering the vast difference in the edge counts among those 5 language domains. For example, the users of German and Russian Wikipedias browsed about the same number of articles, and they did about the same number of hops from websites to articles, shown by the reference counts. But the number of edges representing the clickstreams connecting those visited articles is about 2.7 million in Russian language domain, compared to 5.3 million in German language domain. This suggests that the users in the Russian language domain tended to take the same paths when browsing Wikipedia, while German language domain users took more diverse paths. This difference between German and Russian language domain clickstreams is also reflected in the average number of references per edge, as shown in the bar chart below.
&lt;div class="centered" markdown="1"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream average references per edge by language domain&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;/div&gt;
&lt;div class="centered" markdown="1"&gt;
&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_graph_avg_references_per_edge_barchart.png"&gt;
&lt;/div&gt;
This is the first bar chart in this post where the English domain Wikipedia does not tower over the other language domains. In fact, the Japanese language domain Wikipedia takes the cake here with an average of 333 references per edge, followed by English, Russian, Spanish and Chinese Wikipedias hanging around 250 references per edge, followed by Persian Wikipedia at about 200 references per edge, and the remaining language domains at about 150 references per edge.  &lt;/p&gt;
&lt;p&gt;This lack of a clear pattern is interesting, because if we assume that people tend to browse large and well established Wikipedias more than others, we might expect higher traffic per edge in the larger language domain Wikipedias. On the other hand, if we assume that people browse approximately proportionally for about the same topics across the language domains, we might expect a more uniform distribution. These patterns are just extreme examples, and neither one of them shows up in the data. We will explore these differences further when we examine the reference distributions individually for each of the language domains.  &lt;/p&gt;
&lt;p&gt;Overall, we can see that the larger clickstream traffic is associated with larger Wikipedias in the scatterplot below, although it's not a perfect association and we only have 11 language domains to observe. 
&lt;div class="centered" markdown="1"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream references vs graph size&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;/div&gt;
&lt;div class="centered" markdown="1"&gt;
&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_references_vs_articles_scatterplot.png"&gt;
&lt;/div&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;And if we compare average references per edge to Wikipedia sizes, once again we see no clear relationship.
&lt;div class="centered" markdown="1"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream average references per edge vs graph size&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;/div&gt;
&lt;div class="centered" markdown="1"&gt;
&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_avg_references_per_edge_vs_articles_scatterplot.png"&gt;
&lt;/div&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Looking at graph sizes is the first step in network analysis, and already we can see lots of differences across Wikipedia language domains that would be interesting to investigate further. Before we do that, let's put our Wikipedia clickstream data into some context. The Wikipedia clickstream graph sizes we've seen above cover only the parts of Wikipedia language domains browsed by users in March 2018. Taking a look at how the Wikipedia clickstream graph sizes compare to the entire Wikipedias can tell us how heavily those Wikipedias were utilized in March 2018.&lt;/p&gt;
&lt;h2&gt;Comparing our Wikipedia clickstream data to the entire Wikipedia&lt;/h2&gt;
&lt;h2&gt;External source webpages&lt;/h2&gt;
&lt;h2&gt;Article-to-article subgraph&lt;/h2&gt;
&lt;h1&gt;Next steps&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Hypotheses testing&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;h2&gt;Wikipedia clickstream data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data source/download: &lt;a href="https://dumps.wikimedia.org/other/clickstream/"&gt;Wikipedia clickstream data dumps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data description: &lt;a href="https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream"&gt;Research:Wikipedia clickstream&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;The data is released and maintained by the Wikimedia Foundation's &lt;a href="https://wikitech.wikimedia.org/wiki/Analytics"&gt;Analytics Engineering team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Neo4j graph database&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/graph-database/"&gt;What is a graph database?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/download/"&gt;Download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/guide-neo4j-browser/"&gt;Intro to neo4j's browser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nicolewhite.github.io/neo4j-jupyter/hello-world.html"&gt;Nicole White's Hello-World with jupyter and py2neo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/developer-manual/current/cypher/"&gt;Cypher documentation&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/guide-data-modeling/"&gt;Guide to graph data modeling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;This project&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/33eyes/wiki-clickstream-graph"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/33eyes/wiki-clickstream-graph/blob/master/00_import_data_into_neo4j.ipynb"&gt;Data import jupyter notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="graph-databases"></category><category term="neo4j"></category><category term="Cypher"></category><category term="clickstream"></category><category term="ETL"></category><category term="network-analysis"></category><category term="Python"></category></entry><entry><title>Exploring Wikipedia clickstream data, part 1</title><link href="http://33eyes.github.io/2018-4-16-Wikipedia-clickstream-1.html" rel="alternate"></link><published>2018-04-16T10:20:00-04:00</published><updated>2018-04-16T10:20:00-04:00</updated><author><name>Arina Igumenshcheva</name></author><id>tag:33eyes.github.io,2018-04-16:/2018-4-16-Wikipedia-clickstream-1.html</id><summary type="html">&lt;p&gt;Whenever I want to learn about something, I google it, and sooner or later I end up on Wikipedia. And so do millions of other people, in many different languages. Wikipedia is a vast repository of knowledge organized into interlinked articles, and there are lots of ways to navigate through …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Whenever I want to learn about something, I google it, and sooner or later I end up on Wikipedia. And so do millions of other people, in many different languages. Wikipedia is a vast repository of knowledge organized into interlinked articles, and there are lots of ways to navigate through it as we learn. Whenever we browse Wikipedia, we learn and gather information, so perhaps our browsing behavior tendencies on Wikipedia can tell us something about how we learn on the internet. Do we tend to get the info that we need and move on? Do we delve into details and subtopics when we find something interesting? Do we tend to wander off into other topics? Do these behaviors vary across the different Wikipedia language domains? We can explore such questions by looking at the Wikipedia clickstream data.&lt;/p&gt;
&lt;h2&gt;What is Wikipedia clickstream data?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Clickstream"&gt;Clickstream&lt;/a&gt; data in general is data about which webpages a website user visits. The Wikipedia clickstream data we will be looking at contains the paths Wikipedia users took from article to article on Wikipedia, along with general categories for webpages from which they came to Wikipedia. The Wikipedia clickstream data is released at aggregate level, summing up all user paths per webpage-article pair.  &lt;/p&gt;
&lt;p&gt;In this project, I will explore the Wikipedia clickstream data across multiple language domains. Due to the highly interconnected nature of the data, I will use a graph database for data modeling, storage and manipulation, and network analysis for deriving insights from the data. I will be using the &lt;a href="https://neo4j.com/"&gt;neo4j&lt;/a&gt; graph database along with python.  &lt;/p&gt;
&lt;p&gt;The goals of this project are to explore clickstream data as a graph in general using network analysis techniques, and to see if there are any interesting patterns or differences in the way Wikipedia is used across different language domains.  &lt;/p&gt;
&lt;h2&gt;The dataset&lt;/h2&gt;
&lt;p&gt;In 2015, Wikipedia started releasing datasets of clickstream counts to Wikipedia articles for research purposes. The project has evolved over time, and as of April 2018 it takes the form of monthly automatic releases in 11 Wikipedia language domains: English, German, French, Spanish, Russian, Italian, Japanese, Portuguese, Polish, Chinese and Persian.  &lt;/p&gt;
&lt;p&gt;The clickstream datasets contain counts of times someone went to a Wikipedia article from either another Wikipedia article or from some other webpage. The destination Wikipedia article here is called a &lt;strong&gt;resource&lt;/strong&gt;, and the webpage from which the user went to the resource is called a &lt;strong&gt;referer&lt;/strong&gt; (yep, &lt;a href="https://en.wikipedia.org/wiki/HTTP_referer"&gt;it's referer and not referrer&lt;/a&gt;). Referers can be another Wikipedia article from the same language domain, a Wikimedia page that is not part of that Wikipedia language domain, a search engine, or some other webpage. If the referer is a Wikipedia article from the same language domain, then the article name is given. If not, then a general referer category is given.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Some examples of the data records:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Article-to-article data record&lt;/em&gt;&lt;br&gt;
    In the English Wikipedia, the article 'Suki Waterhouse' links to the article 'List of Divergent characters'. In March 2018, users went from the 'Suki Waterhouse' article to the 'List of Divergent characters' article 86 times. This clickstream activity between the two articles is recorded in the 2018-03 English Wikipedia clickstream data release as:  &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;referer&lt;/th&gt;
&lt;th&gt;resource&lt;/th&gt;
&lt;th&gt;reference type&lt;/th&gt;
&lt;th&gt;count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://en.wikipedia.org/wiki/Suki_Waterhouse"&gt;Suki_Waterhouse&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://en.wikipedia.org/wiki/List_of_Divergent_characters"&gt;List_of_Divergent_characters&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;link&lt;/td&gt;
&lt;td&gt;86&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;External-source-to-article data record&lt;/em&gt;&lt;br&gt;
    In the English Wikipedia, the article 'Bureau of Investigative Journalism' was visited 18 times during March 2018 from some other Wikimedia page that is not part of the English Wikipedia. This clickstream activity is recorded in the 2018-03 English Wikipedia clickstream data release as:  &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;referer&lt;/th&gt;
&lt;th&gt;resource&lt;/th&gt;
&lt;th&gt;reference type&lt;/th&gt;
&lt;th&gt;count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;other-internal&lt;/td&gt;
&lt;td&gt;&lt;a href="https://en.wikipedia.org/wiki/Bureau_of_Investigative_Journalism"&gt;Bureau_of_Investigative_Journalism&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;external&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Wikipedia clickstream data shows how users get to Wikipedia articles per language domain. It is a weighted network of articles and external sources, weighted by the number of times users went to a given Wikipedia article from either another Wikipedia article or some other webpage.&lt;/p&gt;
&lt;p&gt;For this project I'm using the March 2018 Wikipedia clickstream data release, and all of the 11 language domains provided. I'm restricting the data subset to March 2018 for two reasons. First, the way the Wikipedia clickstream data is processed has evolved significantly over time, and the number of language domains included in the releases has changed as well. The changes in data processing are likely to affect the data analysis results, so I am using the March 2018 subset in order to avoid that. Second, the Wikipedia clickstream data is quite large, and the March 2018 subset gives me enough data for an initial exploration.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;The roadmap for this project&lt;/h2&gt;
&lt;p&gt;The data examples above show that the Wikipedia clickstream data is just a long list of article-to-article or external-webpage-to-article references, along with the number of times those references were made. Many articles refer to many other articles, which makes this data highly interconnected. We can use graph theory and network analysis techniques to analyze this data. And since the clickstream data is pretty large, we'll need to load it into a graph database to get the best network analysis performance.&lt;br&gt;
Here's the plan:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ETL (Export, Transform and Load the data)&lt;/li&gt;
&lt;li&gt;Descriptive statistics and exploratory data analysis&lt;/li&gt;
&lt;li&gt;Let's ask the data some questions!  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This blog post will cover the 1st step, and the next steps will be covered in subsequent posts.&lt;/p&gt;
&lt;h1&gt;ETL&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;(a.k.a. &lt;a href="https://en.wikipedia.org/wiki/Extract,_transform,_load"&gt;Export, Transform and Load&lt;/a&gt; the data)&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Data download and cleaning&lt;/h2&gt;
&lt;p&gt;We can download the March 2018 Wikipedia clickstream data from &lt;a href="https://dumps.wikimedia.org/other/clickstream/2018-03/"&gt;here&lt;/a&gt;. The English Wikipedia clickstream data file is the largest of those, and it unzips to 1.3G. The second largest is the German Wikipedia clickstream, and it unzips to 219M. The data files for the other language domains are much smaller.  &lt;/p&gt;
&lt;p&gt;The Wikipedia clickstream datasets don't need much cleaning on their own.&lt;br&gt;
There are just 4 columns:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;referer webpage (freetext field)&lt;/li&gt;
&lt;li&gt;target article (freetext field)&lt;/li&gt;
&lt;li&gt;reference (categorical text field)&lt;/li&gt;
&lt;li&gt;number of occurrences of the reference (numerical)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That being said, I'm going to import this data into the neo4j graph database, which has its own data import quirks. To address those, I need to clean up the quotes in the freetext fields. Neo4j seems to use quotes as a field terminator, even if you explicitly specify a different one, so the quotes need to be escaped. This can be easily done by running the &lt;code&gt;sed 's/"/\\"/g'&lt;/code&gt; command in terminal for each of the data files. That's it for data cleaning, now we can load the data into a graph database. &lt;/p&gt;
&lt;h2&gt;Data modeling&lt;/h2&gt;
&lt;p&gt;Graph databases are structured differently from RDBMS like MySQL or PotgreSQL, where tables are the key concept. In a graph database, the key concepts are relationships (a.k.a. edges or links) and nodes (a.k.a. vertices or entities), and both of those can have various properties. Before we can load our data into a graph database, we need to decide what are our nodes and relationships in the data.  &lt;/p&gt;
&lt;p&gt;In terms of nodes and relationships, it's quite easy to model the Wikipedia clickstream data: the nodes are the webpages and the relationships are the references. But even in this simple case, there are several variations of this model that we could consider. We have two types of webpages in our data: the Wikipedia articles, where we know the exact IRL, and the external webpages, where we only know a broad category of the webpage, like a search engine, but not the exact webpage. These two types of webpages are very different, and we might want to have different kinds of nodes for them, rather than a single node. Similarly, we have 3 kinds of references from webpage to article in the data. The references could be article-to-article links, external or other, and we could model each one of them as a separate relationship type. There are lots of options and no right answer, it all depends on the use case.&lt;/p&gt;
&lt;p&gt;For this project, I've decided on the following model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nodes:&lt;ul&gt;
&lt;li&gt;Wikipedia articles&lt;ul&gt;
&lt;li&gt;properties: article name, language domain&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;External sources&lt;ul&gt;
&lt;li&gt;properties: external source type (e.g. search engine), language domain &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;relationships:&lt;ul&gt;
&lt;li&gt;REFERRED_TO&lt;ul&gt;
&lt;li&gt;properties: reference type (link, external, other), count of reference occurrences (users went from the referer article to the referred article N times in the data)&lt;/li&gt;
&lt;li&gt;this is a one-directional relationship&lt;/li&gt;
&lt;li&gt;An external source can only refer to articles, and cannot be referred to. An article can refer to other articles, and can be referred to by articles or external sources.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will take a look at what this model looks like in neo4j once we load the data.&lt;/p&gt;
&lt;h2&gt;Importing the data into neo4j&lt;/h2&gt;
&lt;p&gt;Now that we've defined our data model, let's import the clickstream data into neo4j. &lt;a href="https://neo4j.com/"&gt;Neo4j&lt;/a&gt; is a graph database that is both storage- and analytics-friendly. It is currently the most popular graph database, and it has a free community edition. Instead of SQL, neo4j uses the &lt;a href="https://en.wikipedia.org/wiki/Cypher_Query_Language"&gt;Cypher query language&lt;/a&gt; in order to interact with the database. &lt;/p&gt;
&lt;p&gt;There are two ways to import data into neo4j: cypher &lt;code&gt;LOAD CSV&lt;/code&gt; and neo4j's batch importer tool. Neo4j has a nice &lt;a href="https://neo4j.com/developer/guide-import-csv/"&gt;guide&lt;/a&gt; that covers both of them. For this project, I'll import the data using cypher &lt;code&gt;LOAD CSV&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;LOAD CSV&lt;/code&gt; cypher query can be run in either the cypher-shell or from python using the py2neo driver. In both cases, we're running cypher queries against the neo4j database.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Overview of import steps:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Create indexes for any matches or merges that will happen during &lt;code&gt;LOAD CSV&lt;/code&gt;&lt;/em&gt;&lt;br&gt;
    Indexes make finding nodes faster. Since Wikipedia clickstream data rows are webpage-to-article and article-to-article references with lots of articles repeating, when we use &lt;code&gt;LOAD CSV&lt;/code&gt; to load this data into the neo4j database, we will use the &lt;code&gt;MERGE&lt;/code&gt; statement, which checks if a node already exists before creating it. This means that the &lt;code&gt;LOAD CSV&lt;/code&gt; query will search the database for each node to be created. Indexing the nodes beforehand makes this process much faster.&lt;br&gt;
    &lt;br&gt;
    Neo4j automatically manages indices and updates them whenever the graph database has changed. This means that we can set indices before loading the data, and benefit from them at load time. For more info about neo4j indices, see &lt;a href="https://neo4j.com/docs/developer-manual/current/cypher/schema/index/"&gt;neo4j's index documentation&lt;/a&gt;.&lt;br&gt;
    &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cypher query example for setting an index on Article nodes, which are unique by article title and language domain:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;CREATE INDEX ON :Article(title, language_code);
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To see all indices set on a graph, run the following cypher statement:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;CALL db.indexes;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Load nodes&lt;/em&gt;&lt;br&gt;
    While it is possible to load both nodes and relationships in a single &lt;code&gt;LOAD CSV&lt;/code&gt; query, it is often faster to load nodes and relationships in separate queries. To keep the cypher queries simple and easy to read, I'll also load the two types of nodes separately.&lt;br&gt;
    &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cypher query example for loading Article nodes (from internal references only, for now), for English language domain:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;USING PERIODIC COMMIT
LOAD CSV FROM {myfilepath} AS row
FIELDTERMINATOR &amp;#39;\t&amp;#39;
WITH row
WHERE row[2] &amp;amp;lt;&amp;amp;gt; &amp;#39;external&amp;#39;
MERGE (n1:Article {title: row[0], language_code: &amp;#39;EN&amp;#39; })
    ON CREATE SET
        n1.language = &amp;#39;English&amp;#39; 
MERGE (n2:Article {title: row[1], language_code: &amp;#39;EN&amp;#39; })
    ON CREATE SET
        n2.language = &amp;#39;English&amp;#39;
;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Cypher query example for loading ExternalSource and Article nodes (from external references), for English language domain:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;USING PERIODIC COMMIT
LOAD CSV FROM {myfilepath} AS row
FIELDTERMINATOR &amp;#39;\t&amp;#39;
WITH row
WHERE row[2] = &amp;#39;external&amp;#39;
MERGE (n1:ExternalSource {source_type: row[0], language_code: &amp;#39;EN&amp;#39; })
    ON CREATE SET
        n1.language = &amp;#39;English&amp;#39;,
        n1.description = CASE 
            WHEN row[0] = &amp;#39;other-internal&amp;#39; THEN &amp;#39;a page from any other Wikimedia project (not Wikipedia)&amp;#39;
            WHEN row[0] = &amp;#39;other-search&amp;#39; THEN &amp;#39;an external search engine&amp;#39;
            WHEN row[0] = &amp;#39;other-external&amp;#39; THEN &amp;#39;any other external site (not search engine)&amp;#39;
            WHEN row[0] = &amp;#39;other-empty&amp;#39; THEN &amp;#39;an empty referer&amp;#39;
            WHEN row[0] = &amp;#39;other-other&amp;#39; THEN &amp;#39;anything else (catch-all)&amp;#39;
            ELSE &amp;#39;&amp;#39; END

MERGE (n2:Article {title: row[1], language_code: &amp;#39;EN&amp;#39; })
    ON CREATE SET
        n2.language = &amp;#39;English&amp;#39;
;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Load relationships&lt;/em&gt;&lt;br&gt;
    When loading relationships, we simply find the two nodes involved in the relationship in the graph database, and create a relationship record between them.&lt;br&gt;
    &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cypher query example for loading Article-to-Article references, for English language domain:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;USING&lt;/span&gt; &lt;span class="nt"&gt;PERIODIC&lt;/span&gt; &lt;span class="nt"&gt;COMMIT&lt;/span&gt;
&lt;span class="nt"&gt;LOAD&lt;/span&gt; &lt;span class="nt"&gt;CSV&lt;/span&gt; &lt;span class="nt"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;myfilepath&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="nt"&gt;AS&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;
&lt;span class="nt"&gt;FIELDTERMINATOR&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\t&amp;#39;&lt;/span&gt;
&lt;span class="nt"&gt;WITH&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;
&lt;span class="nt"&gt;WHERE&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;external&amp;#39;&lt;/span&gt;
&lt;span class="nt"&gt;MATCH&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;Article&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language_code&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;EN&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;MATCH&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;Article&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language_code&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;EN&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;MERGE&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="nt"&gt;-&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;REFERRED_TO&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="nt"&gt;-&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;(&lt;/span&gt;&lt;span class="nt"&gt;n2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="nt"&gt;ON&lt;/span&gt; &lt;span class="nt"&gt;CREATE&lt;/span&gt; &lt;span class="nt"&gt;SET&lt;/span&gt;
        &lt;span class="nt"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Cypher query example for loading ExternalSource-to-Article references, for English language domain:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;USING&lt;/span&gt; &lt;span class="nt"&gt;PERIODIC&lt;/span&gt; &lt;span class="nt"&gt;COMMIT&lt;/span&gt;
&lt;span class="nt"&gt;LOAD&lt;/span&gt; &lt;span class="nt"&gt;CSV&lt;/span&gt; &lt;span class="nt"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;myfilepath&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="nt"&gt;AS&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;
&lt;span class="nt"&gt;FIELDTERMINATOR&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\t&amp;#39;&lt;/span&gt;
&lt;span class="nt"&gt;WITH&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;
&lt;span class="nt"&gt;WHERE&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;external&amp;#39;&lt;/span&gt;
&lt;span class="nt"&gt;MATCH&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;ExternalSource&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;source_type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language_code&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;EN&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;MATCH&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;Article&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language_code&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;EN&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;MERGE&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="nt"&gt;-&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;REFERRED_TO&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="nt"&gt;-&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;(&lt;/span&gt;&lt;span class="nt"&gt;n2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="nt"&gt;ON&lt;/span&gt; &lt;span class="nt"&gt;CREATE&lt;/span&gt; &lt;span class="nt"&gt;SET&lt;/span&gt;
        &lt;span class="nt"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;When developing &lt;code&gt;LOAD CSV&lt;/code&gt; queries, it is helpful to test them out to see how the data is being processed before writing the data to the graph. This can be done by replacing the &lt;code&gt;MERGE&lt;/code&gt; statement with a &lt;code&gt;RETURN&lt;/code&gt; statement and adding a &lt;code&gt;LIMIT&lt;/code&gt; clause. The &lt;code&gt;RETURN&lt;/code&gt; statement will return the result of the query without writing it to the database, and the &lt;code&gt;LIMIT&lt;/code&gt; clause will limit the returned output to a small number of items for testing.&lt;br&gt;
&lt;br&gt;
Here are some examples:&lt;br&gt;
&lt;br&gt;
This query returns 10 data rows as cypher sees them:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;LOAD CSV FROM {myfilepath} AS row
FIELDTERMINATOR &amp;#39;\t&amp;#39;
RETURN row
LIMIT 10;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This query returns the first field and a newly created description field from 10 data rows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;LOAD CSV FROM {myfilepath} AS row
FIELDTERMINATOR &amp;#39;\t&amp;#39;
WITH row
WHERE row[2] = &amp;#39;external&amp;#39;
RETURN row[0] as n1_source_type,
        CASE 
            WHEN row[0] = &amp;#39;other-internal&amp;#39; THEN &amp;#39;a page from any other Wikimedia project (not Wikipedia)&amp;#39;
            WHEN row[0] = &amp;#39;other-search&amp;#39; THEN &amp;#39;an external search engine&amp;#39;
            WHEN row[0] = &amp;#39;other-external&amp;#39; THEN &amp;#39;any other external site (not search engine)&amp;#39;
            WHEN row[0] = &amp;#39;other-empty&amp;#39; THEN &amp;#39;an empty referer&amp;#39;
            WHEN row[0] = &amp;#39;other-other&amp;#39; THEN &amp;#39;anything else (catch-all)&amp;#39;
            ELSE &amp;#39;&amp;#39; 
        END as n1_description
LIMIT 10;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;    &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="https://github.com/33eyes/wiki-clickstream-graph/blob/master/00_import_data_into_neo4j.ipynb"&gt;Here's my jupyter notebook&lt;/a&gt; with the full import code for March 2018 Wikipedia clickstream data across all language domains using py2neo.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Now that we have loaded the data into neo4j, we can check that our database schema matches our graph data model. The easiest way to explore the data that's been loaded into neo4j is through the &lt;a href="https://neo4j.com/developer/guide-neo4j-browser/"&gt;neo4j browser&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;To view the database schema, run &lt;code&gt;CALL db.schema()&lt;/code&gt; in the neo4j browser.&lt;br&gt;
&lt;br&gt;&lt;br&gt;
&lt;div class="centered" markdown="1"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Wikipedia clickstream db schema&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;/div&gt;
&lt;div class="centered" markdown="1"&gt;
&lt;img alt="Wikipedia clickstream db schema" src="http://33eyes.github.io/images/graph_db_schema.png"&gt;
&lt;/div&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;To view a sample of the data in the graph, we can run &lt;code&gt;MATCH p=()-[r:REFERRED_TO]-&amp;gt;(n:Article {language_code:'EN'}) RETURN p LIMIT 25&lt;/code&gt; in the neo4j browser:&lt;br&gt;
&lt;div class="centered scaled" markdown="1"&gt;
&lt;img alt="neo4j query output" src="http://33eyes.github.io/images/english_wiki_25_rels_graph.png"&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;From this sample subgraph, we can see that the clickstream data is highly interconnected. We can inspect closer a handful of nodes in the neo4j browser on a case by case basis, but to get a sense of what's going on in this graph as a whole, we need to use network analysis techniques.&lt;/p&gt;
&lt;h1&gt;Next steps&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Descriptive statistics and EDA&lt;/li&gt;
&lt;li&gt;Hypotheses testing&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;h2&gt;Wikipedia clickstream data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data source/download: &lt;a href="https://dumps.wikimedia.org/other/clickstream/"&gt;Wikipedia clickstream data dumps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data description: &lt;a href="https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream"&gt;Research:Wikipedia clickstream&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;The data is released and maintained by the Wikimedia Foundation's &lt;a href="https://wikitech.wikimedia.org/wiki/Analytics"&gt;Analytics Engineering team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Neo4j graph database&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/graph-database/"&gt;What is a graph database?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/download/"&gt;Download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/guide-neo4j-browser/"&gt;Intro to neo4j's browser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nicolewhite.github.io/neo4j-jupyter/hello-world.html"&gt;Nicole White's Hello-World with jupyter and py2neo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/developer-manual/current/cypher/"&gt;Cypher documentation&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/guide-data-modeling/"&gt;Guide to graph data modeling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;This project&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/33eyes/wiki-clickstream-graph"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/33eyes/wiki-clickstream-graph/blob/master/00_import_data_into_neo4j.ipynb"&gt;Data import jupyter notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="graph-databases"></category><category term="neo4j"></category><category term="Cypher"></category><category term="clickstream"></category><category term="ETL"></category><category term="network-analysis"></category><category term="Python"></category></entry><entry><title>Blogging with Pelican</title><link href="http://33eyes.github.io/2018-1-25-Blogging-with-Pelican.html" rel="alternate"></link><published>2018-01-25T10:20:00-05:00</published><updated>2018-01-25T10:20:00-05:00</updated><author><name>Arina Igumenshcheva</name></author><id>tag:33eyes.github.io,2018-01-25:/2018-1-25-Blogging-with-Pelican.html</id><summary type="html">&lt;p&gt;Static site generators are a great option for blogging. They are faster, cheaper and more secure than traditional CMS blogging platforms like Wordpress. In addition, static site generators also offer straight-forward version control of the entire site, making it easy to restore earlier versions of the site if needed. There …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Static site generators are a great option for blogging. They are faster, cheaper and more secure than traditional CMS blogging platforms like Wordpress. In addition, static site generators also offer straight-forward version control of the entire site, making it easy to restore earlier versions of the site if needed. There are lots of blog posts out there extolling the benefits of static site generators in great detail, so I will skip repeating them here, and list a few in the reference section below.&lt;/p&gt;
&lt;h2&gt;Why Pelican?&lt;/h2&gt;
&lt;p&gt;So, static site generators are good stuff. There are hundreds of static site generators out there, and developers keep coming up with new ones. Currently, the most popular one is Jekyll. It's simple, Ruby-based, very easy to publish on GitHub, and has a large and active community. Many other static site generators are pretty great as well, and which one you choose depends your preferences and goals. Some static site generators are better suited for blogs, others for documentation, some are more customizable than others, some are heavy on JavaScript, some can be hosted on GitHub or Dropbox, etc. And of course, they are written in a variety of languages, including Ruby, Go, Python and JavaScript. After briefly trying out Jekyll, I've decided to go with a different static site generator for my blog: &lt;a href="https://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Here are my very subjective reasons for choosing Pelican over Jekyll:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pelican is Python-based&lt;/strong&gt;, and Jekyll is Ruby-based. While I'm familiar with both languages, I use Python for data science projects, and this blog is about data science, so I thought it would be easier to stick with Python. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pelican seems to me more customizable than Jekyll&lt;/strong&gt;. Pelican's themes are easy to switch, because they live in a separate folder and you simply point to the theme's folder in your config file to appy it to your website. Pelican also offers plugins.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pelican is more Windows-friendly&lt;/strong&gt;. Local development with Jekyll on my Windows 10 machine did not work for me. Jekyll is so simple that this is not a big problem, because you can easily use GitHub for all your publishing needs with Jekyll, or you can clone your Jekyll blog repo to &lt;a href="https://c9.io"&gt;cloud9&lt;/a&gt; (or some other virtual linux box) and develop it locally there. Local development with Pelican on Windows did work for me, although with some adjustments. While this is not crucial, I consider it a plus. &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Setup&lt;/h1&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;First, make sure that you have &lt;a href="https://git-scm.com/downloads"&gt;git&lt;/a&gt;, &lt;a href="https://www.anaconda.com/download/"&gt;Python&lt;/a&gt; and &lt;a href="https://pip.pypa.io/en/stable/installing/"&gt;pip&lt;/a&gt; installed. If using Windows, I recommend using Git Bash for running the unix terminal commands below (it's included with the git distribution for Windows).&lt;/p&gt;
&lt;h5&gt;Install pelican and markdown&lt;/h5&gt;
&lt;p&gt;Run the following command in your terminal: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install pelican markdown
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Create a project for your blog&lt;/h2&gt;
&lt;p&gt;Create a directory for your new project, &lt;code&gt;cd&lt;/code&gt; into it in your terminal, and run the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pelican-quickstart
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This command will create a skeleton of your pelican project. It will ask you some questions when you run it. You can hit &lt;code&gt;Enter&lt;/code&gt; to pick the default answers where available. All you really need to enter is the website title and author. The settings this script asks for can be adjusted later in the project's config file. &lt;/p&gt;
&lt;h2&gt;Create your first blog post&lt;/h2&gt;
&lt;p&gt;You need to create some content before you can generate your site with pelican. Let's make a quick "hello world!" blog post to get started. In a text editor of your choice, enter something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Title&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Hello&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;world&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2018&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;
&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Test&lt;/span&gt;

&lt;span class="n"&gt;Hello&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;world&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt; &lt;span class="n"&gt;This&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;blog&lt;/span&gt; &lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Save this text file as &lt;code&gt;hello-world.md&lt;/code&gt; in the &lt;code&gt;content&lt;/code&gt; folder of your pelican project.  &lt;/p&gt;
&lt;p&gt;By default, the &lt;code&gt;content&lt;/code&gt; folder is where all of your Markdown content files need to be so that pelican can process them when generating the static site. Chronological content (i.e. blog posts) go into the main &lt;code&gt;content&lt;/code&gt; folder, and if you'd like to have non-chronological content, like an About page, you'll need to create a &lt;code&gt;pages&lt;/code&gt; subfolder inside the &lt;code&gt;content&lt;/code&gt; folder.&lt;/p&gt;
&lt;h2&gt;Generate your static site&lt;/h2&gt;
&lt;p&gt;From your project's folder, run the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pelican content
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This command processes your content inside the &lt;code&gt;content&lt;/code&gt; folder and saves the static webpage versions of your content in the &lt;code&gt;output&lt;/code&gt; folder of your project.&lt;/p&gt;
&lt;h2&gt;Run your site locally&lt;/h2&gt;
&lt;p&gt;There are a couple of options for how to do this in pelican, this is just one of them. &lt;/p&gt;
&lt;p&gt;From your project's folder, run the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;make devserver
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This command starts your local development server. You can view your site on &lt;code&gt;localhost:8000&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;While this command runs in the background and you can still use the same terminal window, it outputs server status messages to that window, which can get confusing if you're trying to run other commands there. So it's easier to just leave this terminal for the server, and do other work in a new terminal window.&lt;/p&gt;
&lt;p&gt;The command for stopping the development server is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;make stopserver
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since the &lt;code&gt;make devserver&lt;/code&gt; command runs in the background, you can't just &lt;code&gt;Ctrl+C&lt;/code&gt; out of it.&lt;/p&gt;
&lt;h2&gt;Customization&lt;/h2&gt;
&lt;h5&gt;Settings&lt;/h5&gt;
&lt;p&gt;Pelican has two settings files in the main project folder: &lt;code&gt;pelicanconf.py&lt;/code&gt; and &lt;code&gt;publishconf.py&lt;/code&gt;. When you run the local development server with &lt;code&gt;make devserver&lt;/code&gt;, pelican uses just the &lt;code&gt;pelicanconf.py&lt;/code&gt; settings file. This is the file you will use most of the time to configure your site. Many of the settings here are used by themes.&lt;/p&gt;
&lt;p&gt;When you are ready to publish your website, you can run &lt;code&gt;make publish&lt;/code&gt;, and pelican will generate the static site using the settings you've specified in &lt;code&gt;pelicanconf.py&lt;/code&gt; and the settings in &lt;code&gt;publishconf.py&lt;/code&gt;. The &lt;code&gt;publishconf.py&lt;/code&gt; settings file imports &lt;code&gt;pelicanconf.py&lt;/code&gt; at the start of the file, and whatever settings you specify in &lt;code&gt;publishconf.py&lt;/code&gt; after the import will overwrite the &lt;code&gt;pelicanconf.py&lt;/code&gt;. One example of using that is setting the &lt;code&gt;SITEURL&lt;/code&gt; prefix. You'll want &lt;code&gt;SITEURL = ''&lt;/code&gt; in your &lt;code&gt;pelicanconf.py&lt;/code&gt; for local development, and &lt;code&gt;SITEURL = 'http://yourusername.github.io'&lt;/code&gt; in your &lt;code&gt;publishconf.py&lt;/code&gt; for publishing your site to GitHub pages.&lt;/p&gt;
&lt;h5&gt;Themes&lt;/h5&gt;
&lt;p&gt;Pelican makes it very easy to change themes. All you need is to have a folder with your pelican theme(s) in your project folder, and then just point to the theme's folder in your &lt;code&gt;pelicanconf.py&lt;/code&gt; file: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;THEME = &amp;quot;./pelican-themes/my-custom-theme&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can clone or download pelican themes from the &lt;a href="https://github.com/getpelican/pelican-themes"&gt;pelican theme repository&lt;/a&gt;, or create your own. &lt;/p&gt;
&lt;h2&gt;Hosting on GitHub Pages&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Create an empty repo on &lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt;, and call it &lt;code&gt;yourusername.github.io&lt;/code&gt;, subbing in your GitHub username. Make sure its visibility is set to public.&lt;/li&gt;
&lt;li&gt;Use git to push the contents of your &lt;code&gt;output&lt;/code&gt; folder onto the &lt;code&gt;master&lt;/code&gt; branch of your GitHub repo.&lt;br&gt;
There are a couple ways to do this:&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ghp-import&lt;/code&gt; is a great plugin if you are on a Mac or unix-based OS, but not so great if you have Windows. There is a &lt;a href="http://docs.getpelican.com/en/stable/tips.html#publishing-to-github"&gt;Windows workaround for this (see the note)&lt;/a&gt;, but it did not work on my machine. All &lt;code&gt;ghp-import&lt;/code&gt; does is create a branch called &lt;code&gt;gh-pages&lt;/code&gt; and put only the contents of your &lt;code&gt;output&lt;/code&gt; folder on it. Then, every time you run it, it refreshes those contents.
To install &lt;code&gt;ghp-import&lt;/code&gt; on a Mac/unix-ish OS, run: &lt;code&gt;pip install ghp-import&lt;/code&gt;. (For Windows, see if the workaround linked above works for you. If not, the alternative way below should work.)
Once that's installed, you can run &lt;code&gt;ghp-import output&lt;/code&gt; to commit the contents of your &lt;code&gt;outout&lt;/code&gt; folder onto the &lt;code&gt;gh-pages&lt;/code&gt; git branch.
After that, push your &lt;code&gt;gh-pages&lt;/code&gt; local branch onto your remote GitHub Pages repo &lt;code&gt;master&lt;/code&gt; branch by running the following: &lt;code&gt;git push git@github.com:yourusername/yourusername.github.io.git gh-pages:master&lt;/code&gt; .  &lt;/li&gt;
&lt;li&gt;Another way to do this is to just copy your &lt;code&gt;output&lt;/code&gt; folder somewhere outside your pelican project folder, initialize it as its own git repository, commit everything to the local &lt;code&gt;master&lt;/code&gt; branch, and push that local &lt;code&gt;master&lt;/code&gt; branch to the remote GitHub Pages repo &lt;code&gt;master&lt;/code&gt; branch. This does mean that you'll have to copy over your &lt;code&gt;output&lt;/code&gt; folder every time you want to publish to GitHub, but it is just one &lt;code&gt;cp&lt;/code&gt; command instead of the &lt;code&gt;ghp-import&lt;/code&gt;, and then remember to push to GitHub from that external &lt;code&gt;output&lt;/code&gt; folder. It's pretty easy to write a script to do this as well, and this approach works fine on Windows.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Note: GitHub might take a while to generate your website when you push it for the first time.&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;My blogging workflow&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;cd&lt;/code&gt; into the pelican project folder, and start up the local development server with &lt;code&gt;make devserver&lt;/code&gt; .&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Write some content / make some customizations&lt;/strong&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the blog posts should go into the &lt;code&gt;content&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;static pages should go into &lt;code&gt;content/pages&lt;/code&gt; subfolder&lt;/li&gt;
&lt;li&gt;blog/theme settings can be adjusted in &lt;code&gt;pelicanconf.py&lt;/code&gt; config file&lt;/li&gt;
&lt;li&gt;themes can be added or modified in &lt;code&gt;pelican-themes&lt;/code&gt; folder  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Review the local changes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;open &lt;code&gt;localhost:8000&lt;/code&gt; in browser and review the site&lt;/li&gt;
&lt;li&gt;once happy with the local changes, shut down the development server with &lt;code&gt;make stopserver&lt;/code&gt; .&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Commit changes to my blog development repo&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Save blog development changes to a separate repo.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generate production output&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;run &lt;code&gt;make publish&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Push the new static output to my GitHub Pages repo&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;copy the contents of &lt;code&gt;output&lt;/code&gt; folder (either to the &lt;code&gt;gh-pages&lt;/code&gt; branch via &lt;code&gt;ghp-import&lt;/code&gt;, or to an external &lt;code&gt;output&lt;/code&gt; folder with its own repo)&lt;/li&gt;
&lt;li&gt;push the new static output to the &lt;code&gt;master&lt;/code&gt; branch of the GitHub Pages repo.   &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;h2&gt;Static site generators&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://davidwalsh.name/introduction-static-site-generators"&gt;A nice intro to static site generators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ihommani.github.io/pelican.html"&gt;Thinking about Pelican&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.formkeep.com/not-sure-which-static-site-generator-to-pick/"&gt;Not Sure Which Static Site Generator to Pick?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Pelican resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://docs.getpelican.com/en/stable/"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getpelican/pelican"&gt;Source code on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.getpelican.com/category/news.html"&gt;Development blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://docs.getpelican.com/en/stable/tips.html#"&gt;Documentation Tips&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Pelican tutorials&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nafiulis.me/making-a-static-blog-with-pelican.html"&gt;Engineering Fantasy - Making a static blog with Pelican&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://archerimagine.com/articles/pelican/cost-effective-blogging-with-Pelican-and-Github.html"&gt;Cost effective blogging with Pelican and GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="blogging"></category><category term="pelican"></category><category term="git"></category><category term="Windows"></category><category term="Python"></category></entry><entry><title>First week at Metis</title><link href="http://33eyes.github.io/2018-1-15-First-week-at-Metis.html" rel="alternate"></link><published>2018-01-15T10:20:00-05:00</published><updated>2018-01-15T10:20:00-05:00</updated><author><name>Arina Igumenshcheva</name></author><id>tag:33eyes.github.io,2018-01-15:/2018-1-15-First-week-at-Metis.html</id><summary type="html">&lt;p&gt;My first week at Metis has just breezed by. While I am not new to data analytics and statistics, I am new to data science and Python. For me, this past week has been a mix of a few things old and familiar, and a lot of things new and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;My first week at Metis has just breezed by. While I am not new to data analytics and statistics, I am new to data science and Python. For me, this past week has been a mix of a few things old and familiar, and a lot of things new and surprizing. We've covered lots of introductory grounds on Git and GitHub, pair programming, Pandas, complexity, Matplotlib, command line basics, Seaborn, and, of course, many more Python topics. Having barely coded 10 lines of Python code before this boot camp, I am now a big fan of Python. It seems to have the capacity to do anything, from data science to web development, and when used with Anaconda and iPython notebooks, it can even replace spreadsheet applications for business analysts.&lt;/p&gt;
&lt;h2&gt;What is Metis?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.thisismetis.com/"&gt;Metis&lt;/a&gt; is an accredited immersive data science bootcamp experience that combines traditional in-class instruction in theory and technique with real-world data project work. At Metis, I have completed the full-time, 12-week Data Science program.&lt;/p&gt;
&lt;h1&gt;Highlights of the week&lt;/h1&gt;
&lt;h2&gt;Git and GitHub&lt;/h2&gt;
&lt;p&gt;Having used git on and off for a few years now, I am fairly comfortable with the basic git commands. But throughout the years I have only ever used one GitHub workflow, which consisted of pushing and merging a feature branch onto a single remote repository on GitHub. The preferred GitHub workflow at Metis is to have two remotes, where one is the original upstream repository, and the other is my fork of that repository. The first GitHub workflow pattern has fewer steps and is a bit easier, but the latter offers greater flexibility and is a bit safer.&lt;/p&gt;
&lt;h2&gt;Pair programming&lt;/h2&gt;
&lt;p&gt;Everyone whom I've asked in my cohort loves pair programming! Almost every morning over the past week, we've paired up to tackle an interesting coding puzzle, code it up and figure out its complexity.&lt;/p&gt;
&lt;h2&gt;Pandas, Matplotlib and Seaborn&lt;/h2&gt;
&lt;p&gt;Pandas is just amazing. It makes importing data from various formats so easy and fast, and has so many built-in features. Matplotlib is handy for plotting graphs inline in Python notebooks, and Seaborn makes beautiful graphs out of the box.&lt;/p&gt;
&lt;h2&gt;Running iPython notebooks on a PC&lt;/h2&gt;
&lt;p&gt;I've decided to try to use a PC throughout the boot camp. As a precaution, I've also set up an Ubuntu virtual machine, in case I hit a wall with Windows and have to switch to a *nix OS. I've read a lot of warnings against running Python on Windows, but decided that many of them are a bit too nebulous and/or outdated to take literally. Those warnings may well turn out to be right, but in this case I would rather establish that empirically than go on faith. From what I've seen so far, Anaconda for Windows, along with Anaconda Navigator, take care of many of the frequent obstacles people encounter when running Python on Windows. I might run into problems with some Python libraries that may not be very Windows-friendly, but I'll try to cross that bridge when I get there. So far, using Python on PC with Anaconda has been smooth sailings (although it does sometimes require being comfortable with Windows terminal).&lt;/p&gt;</content><category term="git"></category></entry></feed>