<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Arina Igumenshcheva - Projects</title><link href="http://33eyes.github.io/" rel="alternate"></link><link href="http://33eyes.github.io/feeds/projects.atom.xml" rel="self"></link><id>http://33eyes.github.io/</id><updated>2018-04-20T10:20:00-04:00</updated><entry><title>Exploring Wikipedia clickstream data, part 2</title><link href="http://33eyes.github.io/2018-4-20-Wikipedia-clickstream-2.html" rel="alternate"></link><published>2018-04-20T10:20:00-04:00</published><updated>2018-04-20T10:20:00-04:00</updated><author><name>Arina Igumenshcheva</name></author><id>tag:33eyes.github.io,2018-04-20:/2018-4-20-Wikipedia-clickstream-2.html</id><summary type="html">&lt;p&gt;In the &lt;a href="http://www.arigu.me/2018-4-16-Wikipedia-clickstream-1.html"&gt;previous blog post&lt;/a&gt;, I've covered what Wikipedia clickstream data is, why we may want to explore it using network analysis, and how to load that data into a neo4j database. Here's a brief recap:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wikipedia clickstream data is data about which Wikipedia articles the users visit, and how â€¦&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;In the &lt;a href="http://www.arigu.me/2018-4-16-Wikipedia-clickstream-1.html"&gt;previous blog post&lt;/a&gt;, I've covered what Wikipedia clickstream data is, why we may want to explore it using network analysis, and how to load that data into a neo4j database. Here's a brief recap:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wikipedia clickstream data is data about which Wikipedia articles the users visit, and how they get to Wikipedia. This data is at aggregate level across users per website-to-article hop.&lt;/li&gt;
&lt;li&gt;The data I am using for this project is the March 2018 Wikipedia clickstream data release. This means that my dataset contains data on how the users traversed Wikipedia during March 2018. &lt;/li&gt;
&lt;li&gt;Wikipedia articles have lots of links to other articles, so the Wikipedia clickstream data is highly interconnected. These connections are best modeled and explored as a graph, and for this purpose we've loaded the data into the neo4j graph database.&lt;/li&gt;
&lt;li&gt;The March 2018 Wikipedia clickstream data release provides separate datasets for 11 Wikipedia language domains. These language domains are separate Wikipedia projects, and their data is not interlinked article-to-article in the data release. As a result, we have loaded the data for each language domain as a separate graph.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, I will simply describe and compare the Wikipedia clickstream data graphs across 11 language domains. Further analyses will be continued in the next post.&lt;/p&gt;
&lt;h1&gt;Network properties&lt;/h1&gt;
&lt;p&gt;Network properties are certain graph attributes that can be calculated to get a sense of the structure of the network. These properties can be used to compare networks to each other and to graph models.  &lt;/p&gt;
&lt;h2&gt;Webpage-to-article clickstream graph&lt;/h2&gt;
&lt;p&gt;First, lets take a look at the entire data that we have loaded into separate graphs per language domains. This data includes both Article nodes and ExternalSource nodes. ExternalSource nodes are just categories of external webpages from which users came to Wikipedia articles, and they tend to be connected to lots of articles.  &lt;/p&gt;
&lt;h3&gt;Graph size&lt;/h3&gt;
&lt;p&gt;Graph size is the number of nodes and/or edges in a network. In the webpage-to-article graph, we have two kinds of nodes: Articles and ExternalSources. Each language domain graph has 5 ExternalSource nodes, and we can calculate the counts of Article nodes and all edges in the graph.  &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;language domain&lt;/th&gt;
&lt;th&gt;articles count&lt;/th&gt;
&lt;th&gt;external sources count&lt;/th&gt;
&lt;th&gt;edges count&lt;/th&gt;
&lt;th&gt;references count&lt;/th&gt;
&lt;th&gt;average references per edge&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English (EN)&lt;/td&gt;
&lt;td&gt;4,636,312&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;25,917,378&lt;/td&gt;
&lt;td&gt;7,209,691,324&lt;/td&gt;
&lt;td&gt;278.18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Russian (RU)&lt;/td&gt;
&lt;td&gt;1,310,230&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2,712,051&lt;/td&gt;
&lt;td&gt;704,827,086&lt;/td&gt;
&lt;td&gt;259.89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;German (DE)&lt;/td&gt;
&lt;td&gt;1,307,048&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;5,305,000&lt;/td&gt;
&lt;td&gt;775,318,583&lt;/td&gt;
&lt;td&gt;146.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;French (FR)&lt;/td&gt;
&lt;td&gt;1,018,507&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;3,545,691&lt;/td&gt;
&lt;td&gt;584,620,558&lt;/td&gt;
&lt;td&gt;164.88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Japanese (JA)&lt;/td&gt;
&lt;td&gt;880,747&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2,295,941&lt;/td&gt;
&lt;td&gt;765,429,220&lt;/td&gt;
&lt;td&gt;333.38&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spanish (ES)&lt;/td&gt;
&lt;td&gt;802,360&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;3,409,055&lt;/td&gt;
&lt;td&gt;915,554,698&lt;/td&gt;
&lt;td&gt;268.57&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Italian (IT)&lt;/td&gt;
&lt;td&gt;698,313&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2,919,343&lt;/td&gt;
&lt;td&gt;476,970,761&lt;/td&gt;
&lt;td&gt;163.38&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Portuguese (PT)&lt;/td&gt;
&lt;td&gt;523,616&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1,773,920&lt;/td&gt;
&lt;td&gt;315,514,276&lt;/td&gt;
&lt;td&gt;177.86&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chinese (ZH)&lt;/td&gt;
&lt;td&gt;497,074&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1,238,296&lt;/td&gt;
&lt;td&gt;291,813,076&lt;/td&gt;
&lt;td&gt;235.66&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Polish (PL)&lt;/td&gt;
&lt;td&gt;493,158&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1,556,704&lt;/td&gt;
&lt;td&gt;202,264,461&lt;/td&gt;
&lt;td&gt;129.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Persian (FA)&lt;/td&gt;
&lt;td&gt;170,187&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;375,565&lt;/td&gt;
&lt;td&gt;74,059,745&lt;/td&gt;
&lt;td&gt;197.20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the graph size summary table above, we can see that the English language domain Wikipedia clickstream graph is the largest, with the highest counts of nodes and edges traversed in March 2018. This makes sense, since the English Wikipedia was the first Wikipedia project, and a large proportion of users browse the web using English.  &lt;/p&gt;
&lt;p&gt;Plotting the language domain graph sizes measured by article count, we can see the dramatic difference between the English domain Wikipedia and all other language domains. The English domain Wikipedia users browsed through 4.6 million articles in March 2018. Russian and German domain Wikipedia users have browsed 1.3 million articles each, followed by French, Japanese, Spanish and Italian domain users who have browsed in the 0.7 to 1 million articles range each, followed by Portuguese, Chinese and Polish domain Wikipedia browsing of about half a million articles each. After that, there's another proportionally large drop off in articles browsed in the Persian domain Wikipedia, followed by all other language domain Wikipedias not listed here, since we only have the clickstream data for the largest 11 language domains.  &lt;/p&gt;
&lt;div class="centered"&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream graph sizes by language domain&lt;/strong&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;div class="centered"&gt;
&lt;p&gt;&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_graph_size_barchart.png"&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;If we look at the counts of edges as an additional measure of graph sizes, we see a very similar pattern overall, with the exception of Russian and Japanese language domains, which seem to have relatively low edge counts given their article counts, compared to other language domains. &lt;br&gt;
&lt;div class="centered" markdown="1"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream graph edge counts by language domain&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;/div&gt;
&lt;div class="centered" markdown="1"&gt;
&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_graph_edges_count_barchart.png"&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Edges connect Wikipedia article nodes, and the number of all possible directed edges in a graph is &lt;strong&gt;N(N-1)&lt;/strong&gt;, where &lt;strong&gt;N&lt;/strong&gt; is the number of nodes. The more article nodes we have in a graph, the more edges we can have. But different user clickstream behaviors can result in graph structures with higher or lower actual edge counts relative to the number of all possible edges. So the relative dips in edge counts may suggest structural differences in graphs and underlying user clickstream behaviors.&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Now let's take a look at the distribution of references across the 11 language domains. References here are the individual hops by users from a webpage (an external website or a Wikipedia article) to a Wikipedia article, and their counts are the weights on the edges of our 11 weighted graphs. Again, we'd expect there to be more references in the language domain graphs that have more edges. &lt;/p&gt;
&lt;div class="centered"&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream reference counts by language domain&lt;/strong&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;div class="centered"&gt;
&lt;p&gt;&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_graph_references_count_barchart.png"&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As expected, the references count for English domain Wikipedia is much greater than for the other language domains. But if we look at the next top 5 language domains by reference count, they are all approximately in 600-900 million references range, with Spanish Wikipedia having the 2nd highest references count of 916 million. This is surprisingly uniform considering the vast difference in the edge counts among those 5 language domains. For example, the users of German and Russian Wikipedias browsed about the same number of articles, and they did about the same number of hops from websites to articles, shown by the reference counts. But the number of edges representing the clickstreams connecting those visited articles is about 2.7 million in Russian language domain, compared to 5.3 million in German language domain. This suggests that the users in the Russian language domain tended to take the same paths when browsing Wikipedia, while German language domain users took more diverse paths. This difference between German and Russian language domain clickstreams is also reflected in the average number of references per edge, as shown in the bar chart below.
&lt;div class="centered" markdown="1"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream average references per edge by language domain&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;/div&gt;
&lt;div class="centered" markdown="1"&gt;
&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_graph_avg_references_per_edge_barchart.png"&gt;
&lt;/div&gt;
This is the first bar chart in this post where the English domain Wikipedia does not tower over the other language domains. In fact, the Japanese language domain Wikipedia takes the cake here with an average of 333 references per edge, followed by English, Russian, Spanish and Chinese Wikipedias hanging around 250 references per edge, followed by Persian Wikipedia at about 200 references per edge, and the remaining language domains at about 150 references per edge.  &lt;/p&gt;
&lt;p&gt;This lack of a clear pattern is interesting, because if we assume that people tend to browse large and well established Wikipedias more than others, we might expect higher traffic per edge in the larger language domain Wikipedias. On the other hand, if we assume that people browse approximately proportionally for about the same topics across the language domains, we might expect a more uniform distribution. These patterns are just extreme examples, and neither one of them shows up in the data. We will explore these differences further when we examine the reference distributions individually for each of the language domains.  &lt;/p&gt;
&lt;p&gt;Overall, we can see that the larger clickstream traffic is associated with larger Wikipedias in the scatterplot below, although it's not a perfect association and we only have 11 language domains to observe. 
&lt;div class="centered" markdown="1"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream references vs graph size&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;/div&gt;
&lt;div class="centered" markdown="1"&gt;
&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_references_vs_articles_scatterplot.png"&gt;
&lt;/div&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;And if we compare average references per edge to Wikipedia sizes, once again we see no clear relationship.
&lt;div class="centered" markdown="1"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Webpage-to-article clickstream average references per edge vs graph size&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;/div&gt;
&lt;div class="centered" markdown="1"&gt;
&lt;img alt="barchart" src="http://33eyes.github.io/images/w2a_avg_references_per_edge_vs_articles_scatterplot.png"&gt;
&lt;/div&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Looking at graph sizes is the first step in network analysis, and already we can see lots of differences across Wikipedia language domains that would be interesting to investigate further. Before we do that, let's put our Wikipedia clickstream data into some context. The Wikipedia clickstream graph sizes we've seen above cover only the parts of Wikipedia language domains browsed by users in March 2018. Taking a look at how the Wikipedia clickstream graph sizes compare to the entire Wikipedias can tell us how heavily those Wikipedias were utilized in March 2018.&lt;/p&gt;
&lt;h2&gt;Comparing our Wikipedia clickstream data to the entire Wikipedia&lt;/h2&gt;
&lt;h2&gt;External source webpages&lt;/h2&gt;
&lt;h2&gt;Article-to-article subgraph&lt;/h2&gt;
&lt;h1&gt;Next steps&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Hypotheses testing&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;h2&gt;Wikipedia clickstream data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data source/download: &lt;a href="https://dumps.wikimedia.org/other/clickstream/"&gt;Wikipedia clickstream data dumps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data description: &lt;a href="https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream"&gt;Research:Wikipedia clickstream&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;The data is released and maintained by the Wikimedia Foundation's &lt;a href="https://wikitech.wikimedia.org/wiki/Analytics"&gt;Analytics Engineering team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Neo4j graph database&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/graph-database/"&gt;What is a graph database?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/download/"&gt;Download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/guide-neo4j-browser/"&gt;Intro to neo4j's browser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nicolewhite.github.io/neo4j-jupyter/hello-world.html"&gt;Nicole White's Hello-World with jupyter and py2neo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/developer-manual/current/cypher/"&gt;Cypher documentation&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/guide-data-modeling/"&gt;Guide to graph data modeling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;This project&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/33eyes/wiki-clickstream-graph"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/33eyes/wiki-clickstream-graph/blob/master/00_import_data_into_neo4j.ipynb"&gt;Data import jupyter notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="graph-databases"></category><category term="neo4j"></category><category term="Cypher"></category><category term="clickstream"></category><category term="ETL"></category><category term="network-analysis"></category><category term="Python"></category></entry><entry><title>Exploring Wikipedia clickstream data, part 1</title><link href="http://33eyes.github.io/2018-4-16-Wikipedia-clickstream-1.html" rel="alternate"></link><published>2018-04-16T10:20:00-04:00</published><updated>2018-04-16T10:20:00-04:00</updated><author><name>Arina Igumenshcheva</name></author><id>tag:33eyes.github.io,2018-04-16:/2018-4-16-Wikipedia-clickstream-1.html</id><summary type="html">&lt;p&gt;Whenever I want to learn about something, I google it, and sooner or later I end up on Wikipedia. And so do millions of other people, in many different languages. Wikipedia is a vast repository of knowledge organized into interlinked articles, and there are lots of ways to navigate through â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Whenever I want to learn about something, I google it, and sooner or later I end up on Wikipedia. And so do millions of other people, in many different languages. Wikipedia is a vast repository of knowledge organized into interlinked articles, and there are lots of ways to navigate through it as we learn. Whenever we browse Wikipedia, we learn and gather information, so perhaps our browsing behavior tendencies on Wikipedia can tell us something about how we learn on the internet. Do we tend to get the info that we need and move on? Do we delve into details and subtopics when we find something interesting? Do we tend to wander off into other topics? Do these behaviors vary across the different Wikipedia language domains? We can explore such questions by looking at the Wikipedia clickstream data.&lt;/p&gt;
&lt;h2&gt;What is Wikipedia clickstream data?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Clickstream"&gt;Clickstream&lt;/a&gt; data in general is data about which webpages a website user visits. The Wikipedia clickstream data we will be looking at contains the paths Wikipedia users took from article to article on Wikipedia, along with general categories for webpages from which they came to Wikipedia. The Wikipedia clickstream data is released at aggregate level, summing up all user paths per webpage-article pair.  &lt;/p&gt;
&lt;p&gt;In this project, I will explore the Wikipedia clickstream data across multiple language domains. Due to the highly interconnected nature of the data, I will use a graph database for data modeling, storage and manipulation, and network analysis for deriving insights from the data. I will be using the &lt;a href="https://neo4j.com/"&gt;neo4j&lt;/a&gt; graph database along with python.  &lt;/p&gt;
&lt;p&gt;The goals of this project are to explore clickstream data as a graph in general using network analysis techniques, and to see if there are any interesting patterns or differences in the way Wikipedia is used across different language domains.  &lt;/p&gt;
&lt;h2&gt;The dataset&lt;/h2&gt;
&lt;p&gt;In 2015, Wikipedia started releasing datasets of clickstream counts to Wikipedia articles for research purposes. The project has evolved over time, and as of April 2018 it takes the form of monthly automatic releases in 11 Wikipedia language domains: English, German, French, Spanish, Russian, Italian, Japanese, Portuguese, Polish, Chinese and Persian.  &lt;/p&gt;
&lt;p&gt;The clickstream datasets contain counts of times someone went to a Wikipedia article from either another Wikipedia article or from some other webpage. The destination Wikipedia article here is called a &lt;strong&gt;resource&lt;/strong&gt;, and the webpage from which the user went to the resource is called a &lt;strong&gt;referer&lt;/strong&gt; (yep, &lt;a href="https://en.wikipedia.org/wiki/HTTP_referer"&gt;it's referer and not referrer&lt;/a&gt;). Referers can be another Wikipedia article from the same language domain, a Wikimedia page that is not part of that Wikipedia language domain, a search engine, or some other webpage. If the referer is a Wikipedia article from the same language domain, then the article name is given. If not, then a general referer category is given.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Some examples of the data records:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Article-to-article data record&lt;/em&gt;&lt;br&gt;
    In the English Wikipedia, the article 'Suki Waterhouse' links to the article 'List of Divergent characters'. In March 2018, users went from the 'Suki Waterhouse' article to the 'List of Divergent characters' article 86 times. This clickstream activity between the two articles is recorded in the 2018-03 English Wikipedia clickstream data release as:  &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;referer&lt;/th&gt;
&lt;th&gt;resource&lt;/th&gt;
&lt;th&gt;reference type&lt;/th&gt;
&lt;th&gt;count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://en.wikipedia.org/wiki/Suki_Waterhouse"&gt;Suki_Waterhouse&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://en.wikipedia.org/wiki/List_of_Divergent_characters"&gt;List_of_Divergent_characters&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;link&lt;/td&gt;
&lt;td&gt;86&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;External-source-to-article data record&lt;/em&gt;&lt;br&gt;
    In the English Wikipedia, the article 'Bureau of Investigative Journalism' was visited 18 times during March 2018 from some other Wikimedia page that is not part of the English Wikipedia. This clickstream activity is recorded in the 2018-03 English Wikipedia clickstream data release as:  &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;referer&lt;/th&gt;
&lt;th&gt;resource&lt;/th&gt;
&lt;th&gt;reference type&lt;/th&gt;
&lt;th&gt;count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;other-internal&lt;/td&gt;
&lt;td&gt;&lt;a href="https://en.wikipedia.org/wiki/Bureau_of_Investigative_Journalism"&gt;Bureau_of_Investigative_Journalism&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;external&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Wikipedia clickstream data shows how users get to Wikipedia articles per language domain. It is a weighted network of articles and external sources, weighted by the number of times users went to a given Wikipedia article from either another Wikipedia article or some other webpage.&lt;/p&gt;
&lt;p&gt;For this project I'm using the March 2018 Wikipedia clickstream data release, and all of the 11 language domains provided. I'm restricting the data subset to March 2018 for two reasons. First, the way the Wikipedia clickstream data is processed has evolved significantly over time, and the number of language domains included in the releases has changed as well. The changes in data processing are likely to affect the data analysis results, so I am using the March 2018 subset in order to avoid that. Second, the Wikipedia clickstream data is quite large, and the March 2018 subset gives me enough data for an initial exploration.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;The roadmap for this project&lt;/h2&gt;
&lt;p&gt;The data examples above show that the Wikipedia clickstream data is just a long list of article-to-article or external-webpage-to-article references, along with the number of times those references were made. Many articles refer to many other articles, which makes this data highly interconnected. We can use graph theory and network analysis techniques to analyze this data. And since the clickstream data is pretty large, we'll need to load it into a graph database to get the best network analysis performance.&lt;br&gt;
Here's the plan:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ETL (Export, Transform and Load the data)&lt;/li&gt;
&lt;li&gt;Descriptive statistics and exploratory data analysis&lt;/li&gt;
&lt;li&gt;Let's ask the data some questions!  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This blog post will cover the 1st step, and the next steps will be covered in subsequent posts.&lt;/p&gt;
&lt;h1&gt;ETL&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;(a.k.a. &lt;a href="https://en.wikipedia.org/wiki/Extract,_transform,_load"&gt;Export, Transform and Load&lt;/a&gt; the data)&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Data download and cleaning&lt;/h2&gt;
&lt;p&gt;We can download the March 2018 Wikipedia clickstream data from &lt;a href="https://dumps.wikimedia.org/other/clickstream/2018-03/"&gt;here&lt;/a&gt;. The English Wikipedia clickstream data file is the largest of those, and it unzips to 1.3G. The second largest is the German Wikipedia clickstream, and it unzips to 219M. The data files for the other language domains are much smaller.  &lt;/p&gt;
&lt;p&gt;The Wikipedia clickstream datasets don't need much cleaning on their own.&lt;br&gt;
There are just 4 columns:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;referer webpage (freetext field)&lt;/li&gt;
&lt;li&gt;target article (freetext field)&lt;/li&gt;
&lt;li&gt;reference (categorical text field)&lt;/li&gt;
&lt;li&gt;number of occurrences of the reference (numerical)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That being said, I'm going to import this data into the neo4j graph database, which has its own data import quirks. To address those, I need to clean up the quotes in the freetext fields. Neo4j seems to use quotes as a field terminator, even if you explicitly specify a different one, so the quotes need to be escaped. This can be easily done by running the &lt;code&gt;sed 's/"/\\"/g'&lt;/code&gt; command in terminal for each of the data files. That's it for data cleaning, now we can load the data into a graph database. &lt;/p&gt;
&lt;h2&gt;Data modeling&lt;/h2&gt;
&lt;p&gt;Graph databases are structured differently from RDBMS like MySQL or PotgreSQL, where tables are the key concept. In a graph database, the key concepts are relationships (a.k.a. edges or links) and nodes (a.k.a. vertices or entities), and both of those can have various properties. Before we can load our data into a graph database, we need to decide what are our nodes and relationships in the data.  &lt;/p&gt;
&lt;p&gt;In terms of nodes and relationships, it's quite easy to model the Wikipedia clickstream data: the nodes are the webpages and the relationships are the references. But even in this simple case, there are several variations of this model that we could consider. We have two types of webpages in our data: the Wikipedia articles, where we know the exact IRL, and the external webpages, where we only know a broad category of the webpage, like a search engine, but not the exact webpage. These two types of webpages are very different, and we might want to have different kinds of nodes for them, rather than a single node. Similarly, we have 3 kinds of references from webpage to article in the data. The references could be article-to-article links, external or other, and we could model each one of them as a separate relationship type. There are lots of options and no right answer, it all depends on the use case.&lt;/p&gt;
&lt;p&gt;For this project, I've decided on the following model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nodes:&lt;ul&gt;
&lt;li&gt;Wikipedia articles&lt;ul&gt;
&lt;li&gt;properties: article name, language domain&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;External sources&lt;ul&gt;
&lt;li&gt;properties: external source type (e.g. search engine), language domain &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;relationships:&lt;ul&gt;
&lt;li&gt;REFERRED_TO&lt;ul&gt;
&lt;li&gt;properties: reference type (link, external, other), count of reference occurrences (users went from the referer article to the referred article N times in the data)&lt;/li&gt;
&lt;li&gt;this is a one-directional relationship&lt;/li&gt;
&lt;li&gt;An external source can only refer to articles, and cannot be referred to. An article can refer to other articles, and can be referred to by articles or external sources.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will take a look at what this model looks like in neo4j once we load the data.&lt;/p&gt;
&lt;h2&gt;Importing the data into neo4j&lt;/h2&gt;
&lt;p&gt;Now that we've defined our data model, let's import the clickstream data into neo4j. &lt;a href="https://neo4j.com/"&gt;Neo4j&lt;/a&gt; is a graph database that is both storage- and analytics-friendly. It is currently the most popular graph database, and it has a free community edition. Instead of SQL, neo4j uses the &lt;a href="https://en.wikipedia.org/wiki/Cypher_Query_Language"&gt;Cypher query language&lt;/a&gt; in order to interact with the database. &lt;/p&gt;
&lt;p&gt;There are two ways to import data into neo4j: cypher &lt;code&gt;LOAD CSV&lt;/code&gt; and neo4j's batch importer tool. Neo4j has a nice &lt;a href="https://neo4j.com/developer/guide-import-csv/"&gt;guide&lt;/a&gt; that covers both of them. For this project, I'll import the data using cypher &lt;code&gt;LOAD CSV&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;LOAD CSV&lt;/code&gt; cypher query can be run in either the cypher-shell or from python using the py2neo driver. In both cases, we're running cypher queries against the neo4j database.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Overview of import steps:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Create indexes for any matches or merges that will happen during &lt;code&gt;LOAD CSV&lt;/code&gt;&lt;/em&gt;&lt;br&gt;
    Indexes make finding nodes faster. Since Wikipedia clickstream data rows are webpage-to-article and article-to-article references with lots of articles repeating, when we use &lt;code&gt;LOAD CSV&lt;/code&gt; to load this data into the neo4j database, we will use the &lt;code&gt;MERGE&lt;/code&gt; statement, which checks if a node already exists before creating it. This means that the &lt;code&gt;LOAD CSV&lt;/code&gt; query will search the database for each node to be created. Indexing the nodes beforehand makes this process much faster.&lt;br&gt;
    &lt;br&gt;
    Neo4j automatically manages indices and updates them whenever the graph database has changed. This means that we can set indices before loading the data, and benefit from them at load time. For more info about neo4j indices, see &lt;a href="https://neo4j.com/docs/developer-manual/current/cypher/schema/index/"&gt;neo4j's index documentation&lt;/a&gt;.&lt;br&gt;
    &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cypher query example for setting an index on Article nodes, which are unique by article title and language domain:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;CREATE INDEX ON :Article(title, language_code);
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To see all indices set on a graph, run the following cypher statement:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;CALL db.indexes;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Load nodes&lt;/em&gt;&lt;br&gt;
    While it is possible to load both nodes and relationships in a single &lt;code&gt;LOAD CSV&lt;/code&gt; query, it is often faster to load nodes and relationships in separate queries. To keep the cypher queries simple and easy to read, I'll also load the two types of nodes separately.&lt;br&gt;
    &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cypher query example for loading Article nodes (from internal references only, for now), for English language domain:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;USING PERIODIC COMMIT
LOAD CSV FROM {myfilepath} AS row
FIELDTERMINATOR &amp;#39;\t&amp;#39;
WITH row
WHERE row[2] &amp;amp;lt;&amp;amp;gt; &amp;#39;external&amp;#39;
MERGE (n1:Article {title: row[0], language_code: &amp;#39;EN&amp;#39; })
    ON CREATE SET
        n1.language = &amp;#39;English&amp;#39; 
MERGE (n2:Article {title: row[1], language_code: &amp;#39;EN&amp;#39; })
    ON CREATE SET
        n2.language = &amp;#39;English&amp;#39;
;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Cypher query example for loading ExternalSource and Article nodes (from external references), for English language domain:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;USING PERIODIC COMMIT
LOAD CSV FROM {myfilepath} AS row
FIELDTERMINATOR &amp;#39;\t&amp;#39;
WITH row
WHERE row[2] = &amp;#39;external&amp;#39;
MERGE (n1:ExternalSource {source_type: row[0], language_code: &amp;#39;EN&amp;#39; })
    ON CREATE SET
        n1.language = &amp;#39;English&amp;#39;,
        n1.description = CASE 
            WHEN row[0] = &amp;#39;other-internal&amp;#39; THEN &amp;#39;a page from any other Wikimedia project (not Wikipedia)&amp;#39;
            WHEN row[0] = &amp;#39;other-search&amp;#39; THEN &amp;#39;an external search engine&amp;#39;
            WHEN row[0] = &amp;#39;other-external&amp;#39; THEN &amp;#39;any other external site (not search engine)&amp;#39;
            WHEN row[0] = &amp;#39;other-empty&amp;#39; THEN &amp;#39;an empty referer&amp;#39;
            WHEN row[0] = &amp;#39;other-other&amp;#39; THEN &amp;#39;anything else (catch-all)&amp;#39;
            ELSE &amp;#39;&amp;#39; END

MERGE (n2:Article {title: row[1], language_code: &amp;#39;EN&amp;#39; })
    ON CREATE SET
        n2.language = &amp;#39;English&amp;#39;
;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Load relationships&lt;/em&gt;&lt;br&gt;
    When loading relationships, we simply find the two nodes involved in the relationship in the graph database, and create a relationship record between them.&lt;br&gt;
    &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cypher query example for loading Article-to-Article references, for English language domain:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;USING&lt;/span&gt; &lt;span class="nt"&gt;PERIODIC&lt;/span&gt; &lt;span class="nt"&gt;COMMIT&lt;/span&gt;
&lt;span class="nt"&gt;LOAD&lt;/span&gt; &lt;span class="nt"&gt;CSV&lt;/span&gt; &lt;span class="nt"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;myfilepath&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="nt"&gt;AS&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;
&lt;span class="nt"&gt;FIELDTERMINATOR&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\t&amp;#39;&lt;/span&gt;
&lt;span class="nt"&gt;WITH&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;
&lt;span class="nt"&gt;WHERE&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;external&amp;#39;&lt;/span&gt;
&lt;span class="nt"&gt;MATCH&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;Article&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language_code&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;EN&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;MATCH&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;Article&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language_code&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;EN&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;MERGE&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="nt"&gt;-&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;REFERRED_TO&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="nt"&gt;-&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;(&lt;/span&gt;&lt;span class="nt"&gt;n2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="nt"&gt;ON&lt;/span&gt; &lt;span class="nt"&gt;CREATE&lt;/span&gt; &lt;span class="nt"&gt;SET&lt;/span&gt;
        &lt;span class="nt"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Cypher query example for loading ExternalSource-to-Article references, for English language domain:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;USING&lt;/span&gt; &lt;span class="nt"&gt;PERIODIC&lt;/span&gt; &lt;span class="nt"&gt;COMMIT&lt;/span&gt;
&lt;span class="nt"&gt;LOAD&lt;/span&gt; &lt;span class="nt"&gt;CSV&lt;/span&gt; &lt;span class="nt"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;myfilepath&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="nt"&gt;AS&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;
&lt;span class="nt"&gt;FIELDTERMINATOR&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\t&amp;#39;&lt;/span&gt;
&lt;span class="nt"&gt;WITH&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;
&lt;span class="nt"&gt;WHERE&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;external&amp;#39;&lt;/span&gt;
&lt;span class="nt"&gt;MATCH&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;ExternalSource&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;source_type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language_code&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;EN&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;MATCH&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;Article&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language_code&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;EN&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;MERGE&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;n1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="nt"&gt;-&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;REFERRED_TO&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="nt"&gt;-&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;(&lt;/span&gt;&lt;span class="nt"&gt;n2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="nt"&gt;ON&lt;/span&gt; &lt;span class="nt"&gt;CREATE&lt;/span&gt; &lt;span class="nt"&gt;SET&lt;/span&gt;
        &lt;span class="nt"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;row&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;When developing &lt;code&gt;LOAD CSV&lt;/code&gt; queries, it is helpful to test them out to see how the data is being processed before writing the data to the graph. This can be done by replacing the &lt;code&gt;MERGE&lt;/code&gt; statement with a &lt;code&gt;RETURN&lt;/code&gt; statement and adding a &lt;code&gt;LIMIT&lt;/code&gt; clause. The &lt;code&gt;RETURN&lt;/code&gt; statement will return the result of the query without writing it to the database, and the &lt;code&gt;LIMIT&lt;/code&gt; clause will limit the returned output to a small number of items for testing.&lt;br&gt;
&lt;br&gt;
Here are some examples:&lt;br&gt;
&lt;br&gt;
This query returns 10 data rows as cypher sees them:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;LOAD CSV FROM {myfilepath} AS row
FIELDTERMINATOR &amp;#39;\t&amp;#39;
RETURN row
LIMIT 10;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This query returns the first field and a newly created description field from 10 data rows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;LOAD CSV FROM {myfilepath} AS row
FIELDTERMINATOR &amp;#39;\t&amp;#39;
WITH row
WHERE row[2] = &amp;#39;external&amp;#39;
RETURN row[0] as n1_source_type,
        CASE 
            WHEN row[0] = &amp;#39;other-internal&amp;#39; THEN &amp;#39;a page from any other Wikimedia project (not Wikipedia)&amp;#39;
            WHEN row[0] = &amp;#39;other-search&amp;#39; THEN &amp;#39;an external search engine&amp;#39;
            WHEN row[0] = &amp;#39;other-external&amp;#39; THEN &amp;#39;any other external site (not search engine)&amp;#39;
            WHEN row[0] = &amp;#39;other-empty&amp;#39; THEN &amp;#39;an empty referer&amp;#39;
            WHEN row[0] = &amp;#39;other-other&amp;#39; THEN &amp;#39;anything else (catch-all)&amp;#39;
            ELSE &amp;#39;&amp;#39; 
        END as n1_description
LIMIT 10;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;    &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="https://github.com/33eyes/wiki-clickstream-graph/blob/master/00_import_data_into_neo4j.ipynb"&gt;Here's my jupyter notebook&lt;/a&gt; with the full import code for March 2018 Wikipedia clickstream data across all language domains using py2neo.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Now that we have loaded the data into neo4j, we can check that our database schema matches our graph data model. The easiest way to explore the data that's been loaded into neo4j is through the &lt;a href="https://neo4j.com/developer/guide-neo4j-browser/"&gt;neo4j browser&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;To view the database schema, run &lt;code&gt;CALL db.schema()&lt;/code&gt; in the neo4j browser.&lt;br&gt;
&lt;br&gt;&lt;br&gt;
&lt;div class="centered" markdown="1"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;strong&gt;Wikipedia clickstream db schema&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;&lt;/div&gt;
&lt;div class="centered" markdown="1"&gt;
&lt;img alt="Wikipedia clickstream db schema" src="http://33eyes.github.io/images/graph_db_schema.png"&gt;
&lt;/div&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;To view a sample of the data in the graph, we can run &lt;code&gt;MATCH p=()-[r:REFERRED_TO]-&amp;gt;(n:Article {language_code:'EN'}) RETURN p LIMIT 25&lt;/code&gt; in the neo4j browser:&lt;br&gt;
&lt;div class="centered scaled" markdown="1"&gt;
&lt;img alt="neo4j query output" src="http://33eyes.github.io/images/english_wiki_25_rels_graph.png"&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;From this sample subgraph, we can see that the clickstream data is highly interconnected. We can inspect closer a handful of nodes in the neo4j browser on a case by case basis, but to get a sense of what's going on in this graph as a whole, we need to use network analysis techniques.&lt;/p&gt;
&lt;h1&gt;Next steps&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Descriptive statistics and EDA&lt;/li&gt;
&lt;li&gt;Hypotheses testing&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;h2&gt;Wikipedia clickstream data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data source/download: &lt;a href="https://dumps.wikimedia.org/other/clickstream/"&gt;Wikipedia clickstream data dumps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data description: &lt;a href="https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream"&gt;Research:Wikipedia clickstream&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;The data is released and maintained by the Wikimedia Foundation's &lt;a href="https://wikitech.wikimedia.org/wiki/Analytics"&gt;Analytics Engineering team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Neo4j graph database&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/graph-database/"&gt;What is a graph database?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/download/"&gt;Download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/guide-neo4j-browser/"&gt;Intro to neo4j's browser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nicolewhite.github.io/neo4j-jupyter/hello-world.html"&gt;Nicole White's Hello-World with jupyter and py2neo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/docs/developer-manual/current/cypher/"&gt;Cypher documentation&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://neo4j.com/developer/guide-data-modeling/"&gt;Guide to graph data modeling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;This project&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/33eyes/wiki-clickstream-graph"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/33eyes/wiki-clickstream-graph/blob/master/00_import_data_into_neo4j.ipynb"&gt;Data import jupyter notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="graph-databases"></category><category term="neo4j"></category><category term="Cypher"></category><category term="clickstream"></category><category term="ETL"></category><category term="network-analysis"></category><category term="Python"></category></entry></feed>